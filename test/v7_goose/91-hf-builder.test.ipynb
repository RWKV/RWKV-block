{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face builder test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose\n",
      "Model file path: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.model/v7-1B5-world.pth\n",
      "Project directory: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block\n",
      "Output directory: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n"
     ]
    }
   ],
   "source": [
    "# Get the current script directory, from the notebook\n",
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "print(\"Notebook directory:\", notebook_dir)\n",
    "\n",
    "model_filename = \"v7-1B5-world\"\n",
    "model_file = os.path.join(notebook_dir, \".model\", f\"{model_filename}.pth\")\n",
    "print(\"Model file path:\", model_file)\n",
    "\n",
    "# Check if the model file exists\n",
    "if os.path.isfile(model_file) is False:\n",
    "    raise Exception(\"Model file does not exist\")\n",
    "\n",
    "# Get the project directory two levels up\n",
    "project_dir = os.path.dirname(os.path.dirname(notebook_dir))\n",
    "print(\"Project directory:\", project_dir)\n",
    "\n",
    "# Output build directory\n",
    "output_dir = os.path.join(notebook_dir, f\".hf_build/{model_filename}/\")\n",
    "print(\"Output directory:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing existing output directory\n",
      "-----------------------------\n",
      "Converting RWKV model to HuggingFace format...\n",
      "Model Class     : v7_goose\n",
      "Model Source    : /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.model/v7-1B5-world.pth\n",
      "Tokenizer Type  : auto\n",
      "Output Directory: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "-----------------------------\n",
      "Building rwkv_block into HF code ...\n",
      "Loading model weights raw state ...\n",
      "Loading model config from weights ...\n",
      "-----------------------------\n",
      "Model Configuration:\n",
      "{'vocab_size': 65536, 'num_hidden_layers': 24, 'hidden_size': 2048, 'hidden_size_att': 2048, 'hidden_size_ffn': 8192, 'head_size': 64, 'tmix_backend': 'auto', 'init_state_wkv': False, 'forward_chunk_size': 4096, 'dropout_rate': 0.0, 'use_cache': True, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': False, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': None, 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 0, 'pad_token_id': None, 'eos_token_id': 0, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '', '_commit_hash': None, '_attn_implementation_internal': None, '_attn_implementation_autoset': False, 'transformers_version': None, 'layer_id': None, 'device': None, 'dtype': None}\n",
      "-----------------------------\n",
      "Loading model class instance ...\n",
      "Detected Tokenizer Type: world\n",
      "Loading model state into class ...\n",
      "-----------------------------\n",
      "Saving tokenizer files ...\n",
      "Saving model code files ...\n",
      "Saving model weight files ...\n",
      "Patching configuration ...\n",
      "-----------------------------\n",
      "Successfully converted RWKV model to HuggingFace format\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Empty the output directory, if it exists\n",
    "if os.path.isdir(output_dir):\n",
    "    import shutil\n",
    "    print(\"Removing existing output directory\")\n",
    "    shutil.rmtree(output_dir)\n",
    "    \n",
    "# Run the hf_builder.py\n",
    "!python3 \"$project_dir/hf_builder/hf_builder.py\" --model_class \"v7_goose\" \"$model_file\" \"$output_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic HELLO WORLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully\n",
      "Running on device: cuda\n",
      "---------------------------------\n",
      "Prompt: HELLO WORLD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: HELLO WORLD!\n",
      "I am a newbie to this forum. I am trying to learn how to use the\n",
      "---------------------------------\n",
      "Prompt: \n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\n",
      "Generated text: \n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\n",
      "The dragons were discovered by a team of scientists led by Dr. John Smith, who was studying\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the built model, using the transformers library\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "# Validating the config and tokenizer are built correctly\n",
    "config = AutoConfig.from_pretrained(output_dir, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir, trust_remote_code=True)\n",
    "\n",
    "# Move the model to the GPU\n",
    "RUN_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Build the model itself\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir, trust_remote_code=True, tmix_backend=\"triton\", device=RUN_DEVICE)\n",
    "model.to(RUN_DEVICE)\n",
    "print(\"Model and tokenizer loaded successfully\")\n",
    "\n",
    "# Print the device being used\n",
    "print(\"Running on device:\", RUN_DEVICE)\n",
    "\n",
    "# Lets generate some text, using the model on the GPU\n",
    "dragon_prompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n",
    "hellow_prompt = \"HELLO WORLD\"\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Prompt: {hellow_prompt}\")\n",
    "inputs = tokenizer(hellow_prompt, return_tensors=\"pt\").to(RUN_DEVICE)\n",
    "outputs = model.generate(**inputs)\n",
    "print(\"Generated text:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Prompt: {dragon_prompt}\")\n",
    "inputs = tokenizer(dragon_prompt, return_tensors=\"pt\").to(RUN_DEVICE)\n",
    "outputs = model.generate(**inputs)\n",
    "print(\"Generated text:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU validation testing (smaller set)\n",
    "**(this is not a substitute for lm-eval-harness : the score is counted differently)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Using HF model tokenizer: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-val-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n"
     ]
    }
   ],
   "source": [
    "# MMLU tester directory\n",
    "mmlu_test_dir = os.path.join(project_dir, \"test/mmlu\")\n",
    "\n",
    "# Run the test dataset builder, optional:  --use_validation_set\n",
    "!python3 {mmlu_test_dir}/BuildTestMMLU.py --hf_model \"$output_dir\" --n_shot 0 --use_validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-val-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : all (count=1531, batches=48) ...\n",
      "Using /home/recursal/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py312_cu121/state_wind_backstepping/build.ninja...\n",
      "/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module state_wind_backstepping...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module state_wind_backstepping...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 170, in <module>\n",
      "    main()\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 164, in main\n",
      "    mmlu_test_runner(\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 73, in mmlu_test_runner\n",
      "    full_logits = forward_func(prompt_id)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 162, in model_forward_for_logits\n",
      "    return model(x).logits\n",
      "           ^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_rwkv7.py\", line 637, in forward\n",
      "    rwkv_outputs = RWKV7Model.forward(\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_rwkv7.py\", line 445, in forward\n",
      "    x_hidden_state, ret_sublist, v_first = block_forward(block, x_hidden_state, prv_stateList[i], v_first)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_rwkv7.py\", line 433, in block_forward\n",
      "    return block(in_x_state, in_rwkv_state, in_v_first)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 2079, in forward\n",
      "    att_out, tmix_shift, tmix_wkv, v_first = self.att(\n",
      "                                             ^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 1889, in forward\n",
      "    xx, wkv_state_out = rwkv7_attn_cuda(r, w, k, v, kk, iclr, s0=wkv_state_in)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 632, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 472, in rwkv7_attn_cuda\n",
      "    chunk_xx, chunk_sT = rwkv7_attn_cuda_chunk(r,w,k,v, kk,iclr, HEAD_SIZE, sT)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 516, in rwkv7_attn_cuda_chunk\n",
      "    xx = CudaWindBackstepping.apply(s1,w,r,k,v,a,b)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/autograd/function.py\", line 575, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 433, in forward\n",
      "    assert all(i.dtype==torch.bfloat16 for i in [w,q,k,v,z,b])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the cuda kernel\n",
    "# Batch size of 32, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 32 --n_shot 0 --use_validation_set --tmix_backend \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-val-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : all (count=1531, batches=48) ...\n",
      "#### all - accuracy=0.3063 , probability=0.2980\n",
      "------------------------------------------------\n",
      "### MMLU overall test result : accuracy=0.3063 , probability=0.2980\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel\n",
    "# Batch size of 32, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 32 --n_shot 0 --use_validation_set --tmix_backend \"triton\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-val-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : all (count=1531, batches=48) ...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 170, in <module>\n",
      "    main()\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 164, in main\n",
      "    mmlu_test_runner(\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 73, in mmlu_test_runner\n",
      "    full_logits = forward_func(prompt_id)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 162, in model_forward_for_logits\n",
      "    return model(x).logits\n",
      "           ^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_rwkv7.py\", line 637, in forward\n",
      "    rwkv_outputs = RWKV7Model.forward(\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_rwkv7.py\", line 445, in forward\n",
      "    x_hidden_state, ret_sublist, v_first = block_forward(block, x_hidden_state, prv_stateList[i], v_first)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_rwkv7.py\", line 433, in block_forward\n",
      "    return block(in_x_state, in_rwkv_state, in_v_first)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 2079, in forward\n",
      "    att_out, tmix_shift, tmix_wkv, v_first = self.att(\n",
      "                                             ^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 1895, in forward\n",
      "    xx, wkv_state_out = rwkv7_attn_fla(r, w, k, v, kk, iclr, BATCH_SIZE, SEQ_LEN, N_HEAD, HEAD_SIZE, xx, wkv_state_in) \n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 535, in rwkv7_attn_fla\n",
      "    output, vk_state = chunk_rwkv7(r=r, log_w=log_w, k=k, v=v, a=a, b=b, initial_state=wkv_state_in.float(), output_final_state=True)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/fla/ops/rwkv7/chunk.py\", line 52, in chunk_rwkv7\n",
      "    return chunk_dplr_delta_rule(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/fla/ops/generalized_delta_rule/dplr/chunk.py\", line 362, in chunk_dplr_delta_rule\n",
      "    o, final_state = ChunkDPLRDeltaRuleFunction.apply(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/autograd/function.py\", line 575, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/fla/utils.py\", line 18, in wrapper\n",
      "    return fn(ctx,\n",
      "           ^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/amp/autocast_mode.py\", line 465, in decorate_fwd\n",
      "    return fwd(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/fla/ops/generalized_delta_rule/dplr/chunk.py\", line 122, in forward\n",
      "    o, final_state = chunk_dplr_fwd(\n",
      "                     ^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/fla/ops/generalized_delta_rule/dplr/chunk.py\", line 77, in chunk_dplr_fwd\n",
      "    o = chunk_dplr_fwd_o(\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/fla/ops/generalized_delta_rule/dplr/chunk_o_fwd.py\", line 121, in chunk_dplr_fwd_o\n",
      "    o = torch.empty_like(v)\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 50.69 MiB is free. Process 2159509 has 3.56 GiB memory in use. Process 2207197 has 3.56 GiB memory in use. Including non-PyTorch memory, this process has 16.46 GiB memory in use. Of the allocated memory 15.61 GiB is allocated by PyTorch, and 398.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel\n",
    "# Batch size of 32, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 32 --n_shot 0 --use_validation_set --tmix_backend \"fla\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU testing \n",
    "**(this is not a substitute for lm-eval-harness : the score is counted differently)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-test-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : abstract_algebra (count=100, batches=4) ...\n",
      "Using /home/recursal/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py312_cu121/state_wind_backstepping/build.ninja...\n",
      "/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module state_wind_backstepping...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module state_wind_backstepping...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 170, in <module>\n",
      "    main()\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 164, in main\n",
      "    mmlu_test_runner(\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 73, in mmlu_test_runner\n",
      "    full_logits = forward_func(prompt_id)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 162, in model_forward_for_logits\n",
      "    return model(x).logits\n",
      "           ^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_rwkv7.py\", line 637, in forward\n",
      "    rwkv_outputs = RWKV7Model.forward(\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_rwkv7.py\", line 445, in forward\n",
      "    x_hidden_state, ret_sublist, v_first = block_forward(block, x_hidden_state, prv_stateList[i], v_first)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_rwkv7.py\", line 433, in block_forward\n",
      "    return block(in_x_state, in_rwkv_state, in_v_first)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 2079, in forward\n",
      "    att_out, tmix_shift, tmix_wkv, v_first = self.att(\n",
      "                                             ^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 1889, in forward\n",
      "    xx, wkv_state_out = rwkv7_attn_cuda(r, w, k, v, kk, iclr, s0=wkv_state_in)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 632, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 472, in rwkv7_attn_cuda\n",
      "    chunk_xx, chunk_sT = rwkv7_attn_cuda_chunk(r,w,k,v, kk,iclr, HEAD_SIZE, sT)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 516, in rwkv7_attn_cuda_chunk\n",
      "    xx = CudaWindBackstepping.apply(s1,w,r,k,v,a,b)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/autograd/function.py\", line 575, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 433, in forward\n",
      "    assert all(i.dtype==torch.bfloat16 for i in [w,q,k,v,z,b])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the cuda kernel\n",
    "# Batch size of 32, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 32 --n_shot 0 --tmix_backend \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-test-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : abstract_algebra (count=100, batches=4) ...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 170, in <module>\n",
      "    main()\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 164, in main\n",
      "    mmlu_test_runner(\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 73, in mmlu_test_runner\n",
      "    full_logits = forward_func(prompt_id)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 162, in model_forward_for_logits\n",
      "    return model(x).logits\n",
      "           ^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_rwkv7.py\", line 637, in forward\n",
      "    rwkv_outputs = RWKV7Model.forward(\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_rwkv7.py\", line 445, in forward\n",
      "    x_hidden_state, ret_sublist, v_first = block_forward(block, x_hidden_state, prv_stateList[i], v_first)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_rwkv7.py\", line 433, in block_forward\n",
      "    return block(in_x_state, in_rwkv_state, in_v_first)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 2079, in forward\n",
      "    att_out, tmix_shift, tmix_wkv, v_first = self.att(\n",
      "                                             ^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 1871, in forward\n",
      "    xx, wkv_state_out = rwkv7_attn_triton(r, w, k, v, kk, iclr, s0=wkv_state_in)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 1133, in rwkv7_attn_triton\n",
      "    return rwkv7_attn_triton_chunk(r,w,k,v, kk,iclr, HEAD_SIZE, dot_prec, s0)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 1173, in rwkv7_attn_triton_chunk\n",
      "    xx, sT = TritonRWKV7.apply(w,r,k,v,a,b,s0,dot_prec)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/autograd/function.py\", line 575, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 757, in forward\n",
      "    s = th.zeros(B,H,T//K,C,C, dtype=th.float32,device=w.device)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 838.69 MiB is free. Process 2159509 has 3.56 GiB memory in use. Process 2207197 has 3.56 GiB memory in use. Including non-PyTorch memory, this process has 15.70 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 1.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel (modified)\n",
    "# Batch size of 32, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 32 --n_shot 0 --tmix_backend \"triton\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-test-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : abstract_algebra (count=100, batches=4) ...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 170, in <module>\n",
      "    main()\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 164, in main\n",
      "    mmlu_test_runner(\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 73, in mmlu_test_runner\n",
      "    full_logits = forward_func(prompt_id)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/RunTestMMLU.py\", line 162, in model_forward_for_logits\n",
      "    return model(x).logits\n",
      "           ^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_rwkv7.py\", line 637, in forward\n",
      "    rwkv_outputs = RWKV7Model.forward(\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_rwkv7.py\", line 445, in forward\n",
      "    x_hidden_state, ret_sublist, v_first = block_forward(block, x_hidden_state, prv_stateList[i], v_first)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_rwkv7.py\", line 433, in block_forward\n",
      "    return block(in_x_state, in_rwkv_state, in_v_first)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 2079, in forward\n",
      "    att_out, tmix_shift, tmix_wkv, v_first = self.att(\n",
      "                                             ^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 1895, in forward\n",
      "    xx, wkv_state_out = rwkv7_attn_fla(r, w, k, v, kk, iclr, BATCH_SIZE, SEQ_LEN, N_HEAD, HEAD_SIZE, xx, wkv_state_in) \n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/.cache/huggingface/modules/transformers_modules/modeling_blocks_rwkv7.py\", line 535, in rwkv7_attn_fla\n",
      "    output, vk_state = chunk_rwkv7(r=r, log_w=log_w, k=k, v=v, a=a, b=b, initial_state=wkv_state_in.float(), output_final_state=True)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/fla/ops/rwkv7/chunk.py\", line 52, in chunk_rwkv7\n",
      "    return chunk_dplr_delta_rule(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/fla/ops/generalized_delta_rule/dplr/chunk.py\", line 362, in chunk_dplr_delta_rule\n",
      "    o, final_state = ChunkDPLRDeltaRuleFunction.apply(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/autograd/function.py\", line 575, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/fla/utils.py\", line 18, in wrapper\n",
      "    return fn(ctx,\n",
      "           ^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/amp/autocast_mode.py\", line 465, in decorate_fwd\n",
      "    return fwd(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/fla/ops/generalized_delta_rule/dplr/chunk.py\", line 122, in forward\n",
      "    o, final_state = chunk_dplr_fwd(\n",
      "                     ^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/fla/ops/generalized_delta_rule/dplr/chunk.py\", line 77, in chunk_dplr_fwd\n",
      "    o = chunk_dplr_fwd_o(\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/fla/ops/generalized_delta_rule/dplr/chunk_o_fwd.py\", line 121, in chunk_dplr_fwd_o\n",
      "    o = torch.empty_like(v)\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 6.69 MiB is free. Process 2159509 has 3.56 GiB memory in use. Process 2207197 has 3.56 GiB memory in use. Including non-PyTorch memory, this process has 16.51 GiB memory in use. Of the allocated memory 15.15 GiB is allocated by PyTorch, and 914.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel (modified)\n",
    "# Batch size of 32, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 32 --n_shot 0 --tmix_backend \"fla\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py-3-12]",
   "language": "python",
   "name": "conda-env-py-3-12-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
