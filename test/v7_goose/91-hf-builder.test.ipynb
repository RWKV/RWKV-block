{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face builder test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose\n",
      "Model file path: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.model/v7-1B5-world.pth\n",
      "Project directory: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block\n",
      "Output directory: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n"
     ]
    }
   ],
   "source": [
    "# Get the current script directory, from the notebook\n",
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "print(\"Notebook directory:\", notebook_dir)\n",
    "\n",
    "model_filename = \"v7-1B5-world\"\n",
    "model_file = os.path.join(notebook_dir, \".model\", f\"{model_filename}.pth\")\n",
    "print(\"Model file path:\", model_file)\n",
    "\n",
    "# Check if the model file exists\n",
    "if os.path.isfile(model_file) is False:\n",
    "    raise Exception(\"Model file does not exist\")\n",
    "\n",
    "# Get the project directory two levels up\n",
    "project_dir = os.path.dirname(os.path.dirname(notebook_dir))\n",
    "print(\"Project directory:\", project_dir)\n",
    "\n",
    "# Output build directory\n",
    "output_dir = os.path.join(notebook_dir, f\".hf_build/{model_filename}/\")\n",
    "print(\"Output directory:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing existing output directory\n",
      "-----------------------------\n",
      "Converting RWKV model to HuggingFace format...\n",
      "Model Class     : v7_goose\n",
      "Model Source    : /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.model/v7-1B5-world.pth\n",
      "Tokenizer Type  : auto\n",
      "Output Directory: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "-----------------------------\n",
      "Building rwkv_block into HF code ...\n",
      "Loading model weights raw state ...\n",
      "Loading model config from weights ...\n",
      "-----------------------------\n",
      "Model Configuration:\n",
      "{'vocab_size': 65536, 'num_hidden_layers': 24, 'hidden_size': 2048, 'hidden_size_att': 2048, 'hidden_size_ffn': 8192, 'head_size': 64, 'tmix_backend': 'auto', 'init_state_wkv': False, 'forward_chunk_size': 4096, 'dropout_rate': 0.0, 'use_cache': True, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': True, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': False, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': None, 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 0, 'pad_token_id': None, 'eos_token_id': 0, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '', '_commit_hash': None, '_attn_implementation_internal': None, '_attn_implementation_autoset': False, 'transformers_version': None, 'layer_id': None, 'device': None, 'dtype': None}\n",
      "-----------------------------\n",
      "Loading model class instance ...\n",
      "Detected Tokenizer Type: world\n",
      "Loading model state into class ...\n",
      "-----------------------------\n",
      "Saving tokenizer files ...\n",
      "Saving model code files ...\n",
      "Saving model weight files ...\n",
      "Patching configuration ...\n",
      "-----------------------------\n",
      "Successfully converted RWKV model to HuggingFace format\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Empty the output directory, if it exists\n",
    "if os.path.isdir(output_dir):\n",
    "    import shutil\n",
    "    print(\"Removing existing output directory\")\n",
    "    shutil.rmtree(output_dir)\n",
    "    \n",
    "# Run the hf_builder.py\n",
    "!python3 \"$project_dir/hf_builder/hf_builder.py\" --model_class \"v7_goose\" \"$model_file\" \"$output_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic HELLO WORLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully\n",
      "Running on device: cuda\n",
      "---------------------------------\n",
      "Prompt: HELLO WORLD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: HELLO WORLD!\n",
      "I am a newbie to this forum. I am trying to learn how to use the\n",
      "---------------------------------\n",
      "Prompt: \n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\n",
      "Generated text: \n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\n",
      "The dragons were discovered by a team of scientists led by Dr. John Smith, who was studying\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the built model, using the transformers library\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "# Validating the config and tokenizer are built correctly\n",
    "config = AutoConfig.from_pretrained(output_dir, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir, trust_remote_code=True)\n",
    "\n",
    "# Move the model to the GPU\n",
    "RUN_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Build the model itself\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir, trust_remote_code=True, tmix_backend=\"triton\", device=RUN_DEVICE)\n",
    "model.to(RUN_DEVICE)\n",
    "print(\"Model and tokenizer loaded successfully\")\n",
    "\n",
    "# Print the device being used\n",
    "print(\"Running on device:\", RUN_DEVICE)\n",
    "\n",
    "# Lets generate some text, using the model on the GPU\n",
    "dragon_prompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n",
    "hellow_prompt = \"HELLO WORLD\"\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Prompt: {hellow_prompt}\")\n",
    "inputs = tokenizer(hellow_prompt, return_tensors=\"pt\").to(RUN_DEVICE)\n",
    "outputs = model.generate(**inputs)\n",
    "print(\"Generated text:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Prompt: {dragon_prompt}\")\n",
    "inputs = tokenizer(dragon_prompt, return_tensors=\"pt\").to(RUN_DEVICE)\n",
    "outputs = model.generate(**inputs)\n",
    "print(\"Generated text:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU validation testing (smaller set)\n",
    "**(this is not a substitute for lm-eval-harness : the score is counted differently)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Using HF model tokenizer: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-val-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n"
     ]
    }
   ],
   "source": [
    "# MMLU tester directory\n",
    "mmlu_test_dir = os.path.join(project_dir, \"test/mmlu\")\n",
    "\n",
    "# Run the test dataset builder, optional:  --use_validation_set\n",
    "!python3 {mmlu_test_dir}/BuildTestMMLU.py --hf_model \"$output_dir\" --n_shot 0 --use_validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-val-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : all (count=1531, batches=48) ...\n",
      "Using /home/recursal/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py312_cu121/state_wind_backstepping/build.ninja...\n",
      "/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module state_wind_backstepping...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module state_wind_backstepping...\n",
      "#### all - accuracy=0.3070 , probability=0.2981\n",
      "------------------------------------------------\n",
      "### MMLU overall test result : accuracy=0.3070 , probability=0.2981\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the cuda kernel\n",
    "# Batch size of 24, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 24 --n_shot 0 --use_validation_set --tmix_backend \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-val-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : all (count=1531, batches=48) ...\n",
      "#### all - accuracy=0.3063 , probability=0.2980\n",
      "------------------------------------------------\n",
      "### MMLU overall test result : accuracy=0.3063 , probability=0.2980\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel\n",
    "# Batch size of 24, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 24 --n_shot 0 --use_validation_set --tmix_backend \"triton\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-val-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : all (count=1531, batches=64) ...\n",
      "#### all - accuracy=0.3083 , probability=0.2983\n",
      "------------------------------------------------\n",
      "### MMLU overall test result : accuracy=0.3083 , probability=0.2983\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel\n",
    "# Batch size of 24, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 24 --n_shot 0 --use_validation_set --tmix_backend \"triton_bighead\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-val-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : all (count=1531, batches=64) ...\n",
      "#### all - accuracy=0.3050 , probability=0.2982\n",
      "------------------------------------------------\n",
      "### MMLU overall test result : accuracy=0.3050 , probability=0.2982\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel\n",
    "# Batch size of 24, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 24 --n_shot 0 --use_validation_set --tmix_backend \"fla\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-val-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : all (count=1531, batches=64) ...\n",
      "#### all - accuracy=0.3057 , probability=0.2982\n",
      "------------------------------------------------\n",
      "### MMLU overall test result : accuracy=0.3057 , probability=0.2982\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel\n",
    "# Batch size of 24, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 24 --n_shot 0 --use_validation_set --tmix_backend \"fla_fused\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU testing \n",
    "**(this is not a substitute for lm-eval-harness : the score is counted differently)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-test-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : abstract_algebra (count=100, batches=5) ...\n",
      "Using /home/recursal/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py312_cu121/state_wind_backstepping/build.ninja...\n",
      "/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module state_wind_backstepping...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module state_wind_backstepping...\n",
      "#### abstract_algebra - accuracy=0.2100 , probability=0.2302\n",
      "### Running MMLU test : anatomy (count=135, batches=6) ...\n",
      "#### anatomy - accuracy=0.2963 , probability=0.3045\n",
      "### Running MMLU test : astronomy (count=152, batches=7) ...\n",
      "#### astronomy - accuracy=0.3882 , probability=0.3346\n",
      "### Running MMLU test : business_ethics (count=100, batches=5) ...\n",
      "#### business_ethics - accuracy=0.2800 , probability=0.2812\n",
      "### Running MMLU test : clinical_knowledge (count=265, batches=12) ...\n",
      "#### clinical_knowledge - accuracy=0.4000 , probability=0.3370\n",
      "### Running MMLU test : college_medicine (count=173, batches=8) ...\n",
      "#### college_medicine - accuracy=0.4104 , probability=0.3393\n",
      "### Running MMLU test : college_biology (count=144, batches=6) ...\n",
      "#### college_biology - accuracy=0.2917 , probability=0.3012\n",
      "### Running MMLU test : college_chemistry (count=100, batches=5) ...\n",
      "#### college_chemistry - accuracy=0.4100 , probability=0.3479\n",
      "### Running MMLU test : college_computer_science (count=100, batches=5) ...\n",
      "#### college_computer_science - accuracy=0.3300 , probability=0.3171\n",
      "### Running MMLU test : college_mathematics (count=100, batches=5) ...\n",
      "#### college_mathematics - accuracy=0.3000 , probability=0.2948\n",
      "### Running MMLU test : college_physics (count=102, batches=5) ...\n",
      "#### college_physics - accuracy=0.3824 , probability=0.3194\n",
      "### Running MMLU test : computer_security (count=100, batches=5) ...\n",
      "#### computer_security - accuracy=0.3100 , probability=0.3020\n",
      "### Running MMLU test : conceptual_physics (count=235, batches=10) ...\n",
      "#### conceptual_physics - accuracy=0.2340 , probability=0.2663\n",
      "### Running MMLU test : econometrics (count=114, batches=5) ...\n",
      "#### econometrics - accuracy=0.2456 , probability=0.2439\n",
      "### Running MMLU test : elementary_mathematics (count=378, batches=16) ...\n",
      "#### elementary_mathematics - accuracy=0.2672 , probability=0.2634\n",
      "### Running MMLU test : electrical_engineering (count=145, batches=7) ...\n",
      "#### electrical_engineering - accuracy=0.2966 , probability=0.2790\n",
      "### Running MMLU test : formal_logic (count=126, batches=6) ...\n",
      "#### formal_logic - accuracy=0.3730 , probability=0.2999\n",
      "### Running MMLU test : global_facts (count=100, batches=5) ...\n",
      "#### global_facts - accuracy=0.1800 , probability=0.2340\n",
      "### Running MMLU test : high_school_european_history (count=165, batches=7) ...\n",
      "#### high_school_european_history - accuracy=0.2848 , probability=0.3126\n",
      "### Running MMLU test : high_school_us_history (count=204, batches=9) ...\n",
      "#### high_school_us_history - accuracy=0.2990 , probability=0.3130\n",
      "### Running MMLU test : high_school_world_history (count=237, batches=10) ...\n",
      "#### high_school_world_history - accuracy=0.2236 , probability=0.2741\n",
      "### Running MMLU test : high_school_geography (count=198, batches=9) ...\n",
      "#### high_school_geography - accuracy=0.4444 , probability=0.3843\n",
      "### Running MMLU test : high_school_government_and_politics (count=193, batches=9) ...\n",
      "#### high_school_government_and_politics - accuracy=0.4249 , probability=0.3964\n",
      "### Running MMLU test : high_school_macroeconomics (count=390, batches=17) ...\n",
      "#### high_school_macroeconomics - accuracy=0.3846 , probability=0.3396\n",
      "### Running MMLU test : high_school_microeconomics (count=238, batches=10) ...\n",
      "#### high_school_microeconomics - accuracy=0.3697 , probability=0.3376\n",
      "### Running MMLU test : high_school_psychology (count=545, batches=23) ...\n",
      "#### high_school_psychology - accuracy=0.4642 , probability=0.3958\n",
      "### Running MMLU test : high_school_biology (count=310, batches=13) ...\n",
      "#### high_school_biology - accuracy=0.3516 , probability=0.3391\n",
      "### Running MMLU test : high_school_chemistry (count=203, batches=9) ...\n",
      "#### high_school_chemistry - accuracy=0.2906 , probability=0.2692\n",
      "### Running MMLU test : high_school_computer_science (count=100, batches=5) ...\n",
      "#### high_school_computer_science - accuracy=0.2300 , probability=0.2626\n",
      "### Running MMLU test : high_school_mathematics (count=270, batches=12) ...\n",
      "#### high_school_mathematics - accuracy=0.2630 , probability=0.2562\n",
      "### Running MMLU test : high_school_physics (count=151, batches=7) ...\n",
      "#### high_school_physics - accuracy=0.3311 , probability=0.3007\n",
      "### Running MMLU test : high_school_statistics (count=216, batches=9) ...\n",
      "#### high_school_statistics - accuracy=0.4722 , probability=0.3589\n",
      "### Running MMLU test : human_aging (count=223, batches=10) ...\n",
      "#### human_aging - accuracy=0.1659 , probability=0.2402\n",
      "### Running MMLU test : human_sexuality (count=131, batches=6) ...\n",
      "#### human_sexuality - accuracy=0.3359 , probability=0.3225\n",
      "### Running MMLU test : international_law (count=121, batches=6) ...\n",
      "#### international_law - accuracy=0.2314 , probability=0.2509\n",
      "### Running MMLU test : jurisprudence (count=108, batches=5) ...\n",
      "#### jurisprudence - accuracy=0.3056 , probability=0.3108\n",
      "### Running MMLU test : logical_fallacies (count=163, batches=7) ...\n",
      "#### logical_fallacies - accuracy=0.2761 , probability=0.2937\n",
      "### Running MMLU test : machine_learning (count=112, batches=5) ...\n",
      "#### machine_learning - accuracy=0.1786 , probability=0.2390\n",
      "### Running MMLU test : management (count=103, batches=5) ...\n",
      "#### management - accuracy=0.5146 , probability=0.4100\n",
      "### Running MMLU test : marketing (count=234, batches=10) ...\n",
      "#### marketing - accuracy=0.3718 , probability=0.3492\n",
      "### Running MMLU test : medical_genetics (count=100, batches=5) ...\n",
      "#### medical_genetics - accuracy=0.3000 , probability=0.2991\n",
      "### Running MMLU test : miscellaneous (count=783, batches=33) ...\n",
      "#### miscellaneous - accuracy=0.3844 , probability=0.3598\n",
      "### Running MMLU test : moral_disputes (count=346, batches=15) ...\n",
      "#### moral_disputes - accuracy=0.2341 , probability=0.2709\n",
      "### Running MMLU test : moral_scenarios (count=895, batches=38) ...\n",
      "#### moral_scenarios - accuracy=0.2737 , probability=0.2610\n",
      "### Running MMLU test : nutrition (count=306, batches=13) ...\n",
      "#### nutrition - accuracy=0.3235 , probability=0.3249\n",
      "### Running MMLU test : philosophy (count=311, batches=13) ...\n",
      "#### philosophy - accuracy=0.3280 , probability=0.3251\n",
      "### Running MMLU test : prehistory (count=324, batches=14) ...\n",
      "#### prehistory - accuracy=0.3056 , probability=0.3171\n",
      "### Running MMLU test : professional_law (count=1534, batches=64) ...\n",
      "#### professional_law - accuracy=0.2458 , probability=0.2574\n",
      "### Running MMLU test : professional_accounting (count=282, batches=12) ...\n",
      "#### professional_accounting - accuracy=0.2482 , probability=0.2570\n",
      "### Running MMLU test : professional_psychology (count=612, batches=26) ...\n",
      "#### professional_psychology - accuracy=0.2663 , probability=0.2794\n",
      "### Running MMLU test : professional_medicine (count=272, batches=12) ...\n",
      "#### professional_medicine - accuracy=0.4522 , probability=0.3618\n",
      "### Running MMLU test : public_relations (count=110, batches=5) ...\n",
      "#### public_relations - accuracy=0.3000 , probability=0.3043\n",
      "### Running MMLU test : security_studies (count=245, batches=11) ...\n",
      "#### security_studies - accuracy=0.4163 , probability=0.3508\n",
      "### Running MMLU test : sociology (count=201, batches=9) ...\n",
      "#### sociology - accuracy=0.3582 , probability=0.3608\n",
      "### Running MMLU test : us_foreign_policy (count=100, batches=5) ...\n",
      "#### us_foreign_policy - accuracy=0.3600 , probability=0.3402\n",
      "### Running MMLU test : virology (count=166, batches=7) ...\n",
      "#### virology - accuracy=0.2651 , probability=0.2900\n",
      "### Running MMLU test : world_religions (count=171, batches=8) ...\n",
      "#### world_religions - accuracy=0.5088 , probability=0.4292\n",
      "------------------------------------------------\n",
      "### MMLU overall test result : accuracy=0.3226 , probability=0.3095\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the cuda kernel\n",
    "# Batch size of 24, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 24 --n_shot 0 --tmix_backend \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-test-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : abstract_algebra (count=100, batches=5) ...\n",
      "#### abstract_algebra - accuracy=0.2100 , probability=0.2306\n",
      "### Running MMLU test : anatomy (count=135, batches=6) ...\n",
      "#### anatomy - accuracy=0.2963 , probability=0.3044\n",
      "### Running MMLU test : astronomy (count=152, batches=7) ...\n",
      "#### astronomy - accuracy=0.3882 , probability=0.3348\n",
      "### Running MMLU test : business_ethics (count=100, batches=5) ...\n",
      "#### business_ethics - accuracy=0.2800 , probability=0.2810\n",
      "### Running MMLU test : clinical_knowledge (count=265, batches=12) ...\n",
      "#### clinical_knowledge - accuracy=0.4000 , probability=0.3365\n",
      "### Running MMLU test : college_medicine (count=173, batches=8) ...\n",
      "#### college_medicine - accuracy=0.4104 , probability=0.3385\n",
      "### Running MMLU test : college_biology (count=144, batches=6) ...\n",
      "#### college_biology - accuracy=0.2917 , probability=0.3009\n",
      "### Running MMLU test : college_chemistry (count=100, batches=5) ...\n",
      "#### college_chemistry - accuracy=0.4100 , probability=0.3478\n",
      "### Running MMLU test : college_computer_science (count=100, batches=5) ...\n",
      "#### college_computer_science - accuracy=0.3300 , probability=0.3163\n",
      "### Running MMLU test : college_mathematics (count=100, batches=5) ...\n",
      "#### college_mathematics - accuracy=0.3000 , probability=0.2960\n",
      "### Running MMLU test : college_physics (count=102, batches=5) ...\n",
      "#### college_physics - accuracy=0.3824 , probability=0.3189\n",
      "### Running MMLU test : computer_security (count=100, batches=5) ...\n",
      "#### computer_security - accuracy=0.3200 , probability=0.3023\n",
      "### Running MMLU test : conceptual_physics (count=235, batches=10) ...\n",
      "#### conceptual_physics - accuracy=0.2298 , probability=0.2662\n",
      "### Running MMLU test : econometrics (count=114, batches=5) ...\n",
      "#### econometrics - accuracy=0.2456 , probability=0.2443\n",
      "### Running MMLU test : elementary_mathematics (count=378, batches=16) ...\n",
      "#### elementary_mathematics - accuracy=0.2672 , probability=0.2630\n",
      "### Running MMLU test : electrical_engineering (count=145, batches=7) ...\n",
      "#### electrical_engineering - accuracy=0.2966 , probability=0.2788\n",
      "### Running MMLU test : formal_logic (count=126, batches=6) ...\n",
      "#### formal_logic - accuracy=0.3810 , probability=0.2999\n",
      "### Running MMLU test : global_facts (count=100, batches=5) ...\n",
      "#### global_facts - accuracy=0.1800 , probability=0.2341\n",
      "### Running MMLU test : high_school_european_history (count=165, batches=7) ...\n",
      "#### high_school_european_history - accuracy=0.2848 , probability=0.3135\n",
      "### Running MMLU test : high_school_us_history (count=204, batches=9) ...\n",
      "#### high_school_us_history - accuracy=0.2941 , probability=0.3123\n",
      "### Running MMLU test : high_school_world_history (count=237, batches=10) ...\n",
      "#### high_school_world_history - accuracy=0.2236 , probability=0.2738\n",
      "### Running MMLU test : high_school_geography (count=198, batches=9) ...\n",
      "#### high_school_geography - accuracy=0.4444 , probability=0.3840\n",
      "### Running MMLU test : high_school_government_and_politics (count=193, batches=9) ...\n",
      "#### high_school_government_and_politics - accuracy=0.4197 , probability=0.3960\n",
      "### Running MMLU test : high_school_macroeconomics (count=390, batches=17) ...\n",
      "#### high_school_macroeconomics - accuracy=0.3846 , probability=0.3394\n",
      "### Running MMLU test : high_school_microeconomics (count=238, batches=10) ...\n",
      "#### high_school_microeconomics - accuracy=0.3697 , probability=0.3381\n",
      "### Running MMLU test : high_school_psychology (count=545, batches=23) ...\n",
      "#### high_school_psychology - accuracy=0.4624 , probability=0.3950\n",
      "### Running MMLU test : high_school_biology (count=310, batches=13) ...\n",
      "#### high_school_biology - accuracy=0.3516 , probability=0.3388\n",
      "### Running MMLU test : high_school_chemistry (count=203, batches=9) ...\n",
      "#### high_school_chemistry - accuracy=0.2906 , probability=0.2691\n",
      "### Running MMLU test : high_school_computer_science (count=100, batches=5) ...\n",
      "#### high_school_computer_science - accuracy=0.2300 , probability=0.2626\n",
      "### Running MMLU test : high_school_mathematics (count=270, batches=12) ...\n",
      "#### high_school_mathematics - accuracy=0.2630 , probability=0.2561\n",
      "### Running MMLU test : high_school_physics (count=151, batches=7) ...\n",
      "#### high_school_physics - accuracy=0.3311 , probability=0.3010\n",
      "### Running MMLU test : high_school_statistics (count=216, batches=9) ...\n",
      "#### high_school_statistics - accuracy=0.4722 , probability=0.3588\n",
      "### Running MMLU test : human_aging (count=223, batches=10) ...\n",
      "#### human_aging - accuracy=0.1614 , probability=0.2407\n",
      "### Running MMLU test : human_sexuality (count=131, batches=6) ...\n",
      "#### human_sexuality - accuracy=0.3359 , probability=0.3235\n",
      "### Running MMLU test : international_law (count=121, batches=6) ...\n",
      "#### international_law - accuracy=0.2314 , probability=0.2503\n",
      "### Running MMLU test : jurisprudence (count=108, batches=5) ...\n",
      "#### jurisprudence - accuracy=0.3056 , probability=0.3096\n",
      "### Running MMLU test : logical_fallacies (count=163, batches=7) ...\n",
      "#### logical_fallacies - accuracy=0.2761 , probability=0.2937\n",
      "### Running MMLU test : machine_learning (count=112, batches=5) ...\n",
      "#### machine_learning - accuracy=0.1786 , probability=0.2403\n",
      "### Running MMLU test : management (count=103, batches=5) ...\n",
      "#### management - accuracy=0.5146 , probability=0.4102\n",
      "### Running MMLU test : marketing (count=234, batches=10) ...\n",
      "#### marketing - accuracy=0.3761 , probability=0.3496\n",
      "### Running MMLU test : medical_genetics (count=100, batches=5) ...\n",
      "#### medical_genetics - accuracy=0.2900 , probability=0.3001\n",
      "### Running MMLU test : miscellaneous (count=783, batches=33) ...\n",
      "#### miscellaneous - accuracy=0.3857 , probability=0.3597\n",
      "### Running MMLU test : moral_disputes (count=346, batches=15) ...\n",
      "#### moral_disputes - accuracy=0.2370 , probability=0.2706\n",
      "### Running MMLU test : moral_scenarios (count=895, batches=38) ...\n",
      "#### moral_scenarios - accuracy=0.2737 , probability=0.2608\n",
      "### Running MMLU test : nutrition (count=306, batches=13) ...\n",
      "#### nutrition - accuracy=0.3268 , probability=0.3251\n",
      "### Running MMLU test : philosophy (count=311, batches=13) ...\n",
      "#### philosophy - accuracy=0.3280 , probability=0.3258\n",
      "### Running MMLU test : prehistory (count=324, batches=14) ...\n",
      "#### prehistory - accuracy=0.3025 , probability=0.3171\n",
      "### Running MMLU test : professional_law (count=1534, batches=64) ...\n",
      "#### professional_law - accuracy=0.2458 , probability=0.2576\n",
      "### Running MMLU test : professional_accounting (count=282, batches=12) ...\n",
      "#### professional_accounting - accuracy=0.2447 , probability=0.2570\n",
      "### Running MMLU test : professional_psychology (count=612, batches=26) ...\n",
      "#### professional_psychology - accuracy=0.2680 , probability=0.2792\n",
      "### Running MMLU test : professional_medicine (count=272, batches=12) ...\n",
      "#### professional_medicine - accuracy=0.4559 , probability=0.3624\n",
      "### Running MMLU test : public_relations (count=110, batches=5) ...\n",
      "#### public_relations - accuracy=0.3000 , probability=0.3035\n",
      "### Running MMLU test : security_studies (count=245, batches=11) ...\n",
      "#### security_studies - accuracy=0.4163 , probability=0.3511\n",
      "### Running MMLU test : sociology (count=201, batches=9) ...\n",
      "#### sociology - accuracy=0.3582 , probability=0.3616\n",
      "### Running MMLU test : us_foreign_policy (count=100, batches=5) ...\n",
      "#### us_foreign_policy - accuracy=0.3500 , probability=0.3401\n",
      "### Running MMLU test : virology (count=166, batches=7) ...\n",
      "#### virology - accuracy=0.2651 , probability=0.2899\n",
      "### Running MMLU test : world_religions (count=171, batches=8) ...\n",
      "#### world_religions - accuracy=0.5146 , probability=0.4291\n",
      "------------------------------------------------\n",
      "### MMLU overall test result : accuracy=0.3225 , probability=0.3095\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel (modified)\n",
    "# Batch size of 24, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 24 --n_shot 0 --tmix_backend \"triton\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-test-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : abstract_algebra (count=100, batches=5) ...\n",
      "#### abstract_algebra - accuracy=0.2100 , probability=0.2307\n",
      "### Running MMLU test : anatomy (count=135, batches=6) ...\n",
      "#### anatomy - accuracy=0.2963 , probability=0.3042\n",
      "### Running MMLU test : astronomy (count=152, batches=7) ...\n",
      "#### astronomy - accuracy=0.3882 , probability=0.3343\n",
      "### Running MMLU test : business_ethics (count=100, batches=5) ...\n",
      "#### business_ethics - accuracy=0.2800 , probability=0.2808\n",
      "### Running MMLU test : clinical_knowledge (count=265, batches=12) ...\n",
      "#### clinical_knowledge - accuracy=0.3962 , probability=0.3370\n",
      "### Running MMLU test : college_medicine (count=173, batches=8) ...\n",
      "#### college_medicine - accuracy=0.4046 , probability=0.3385\n",
      "### Running MMLU test : college_biology (count=144, batches=6) ...\n",
      "#### college_biology - accuracy=0.2986 , probability=0.3009\n",
      "### Running MMLU test : college_chemistry (count=100, batches=5) ...\n",
      "#### college_chemistry - accuracy=0.4100 , probability=0.3472\n",
      "### Running MMLU test : college_computer_science (count=100, batches=5) ...\n",
      "#### college_computer_science - accuracy=0.3300 , probability=0.3164\n",
      "### Running MMLU test : college_mathematics (count=100, batches=5) ...\n",
      "#### college_mathematics - accuracy=0.3000 , probability=0.2950\n",
      "### Running MMLU test : college_physics (count=102, batches=5) ...\n",
      "#### college_physics - accuracy=0.3824 , probability=0.3199\n",
      "### Running MMLU test : computer_security (count=100, batches=5) ...\n",
      "#### computer_security - accuracy=0.3100 , probability=0.3029\n",
      "### Running MMLU test : conceptual_physics (count=235, batches=10) ...\n",
      "#### conceptual_physics - accuracy=0.2298 , probability=0.2657\n",
      "### Running MMLU test : econometrics (count=114, batches=5) ...\n",
      "#### econometrics - accuracy=0.2456 , probability=0.2433\n",
      "### Running MMLU test : elementary_mathematics (count=378, batches=16) ...\n",
      "#### elementary_mathematics - accuracy=0.2672 , probability=0.2631\n",
      "### Running MMLU test : electrical_engineering (count=145, batches=7) ...\n",
      "#### electrical_engineering - accuracy=0.3103 , probability=0.2792\n",
      "### Running MMLU test : formal_logic (count=126, batches=6) ...\n",
      "#### formal_logic - accuracy=0.3730 , probability=0.2997\n",
      "### Running MMLU test : global_facts (count=100, batches=5) ...\n",
      "#### global_facts - accuracy=0.1800 , probability=0.2340\n",
      "### Running MMLU test : high_school_european_history (count=165, batches=7) ...\n",
      "#### high_school_european_history - accuracy=0.2788 , probability=0.3129\n",
      "### Running MMLU test : high_school_us_history (count=204, batches=9) ...\n",
      "#### high_school_us_history - accuracy=0.2941 , probability=0.3127\n",
      "### Running MMLU test : high_school_world_history (count=237, batches=10) ...\n",
      "#### high_school_world_history - accuracy=0.2194 , probability=0.2741\n",
      "### Running MMLU test : high_school_geography (count=198, batches=9) ...\n",
      "#### high_school_geography - accuracy=0.4444 , probability=0.3841\n",
      "### Running MMLU test : high_school_government_and_politics (count=193, batches=9) ...\n",
      "#### high_school_government_and_politics - accuracy=0.4249 , probability=0.3957\n",
      "### Running MMLU test : high_school_macroeconomics (count=390, batches=17) ...\n",
      "#### high_school_macroeconomics - accuracy=0.3846 , probability=0.3394\n",
      "### Running MMLU test : high_school_microeconomics (count=238, batches=10) ...\n",
      "#### high_school_microeconomics - accuracy=0.3697 , probability=0.3381\n",
      "### Running MMLU test : high_school_psychology (count=545, batches=23) ...\n",
      "#### high_school_psychology - accuracy=0.4661 , probability=0.3951\n",
      "### Running MMLU test : high_school_biology (count=310, batches=13) ...\n",
      "#### high_school_biology - accuracy=0.3548 , probability=0.3391\n",
      "### Running MMLU test : high_school_chemistry (count=203, batches=9) ...\n",
      "#### high_school_chemistry - accuracy=0.2906 , probability=0.2687\n",
      "### Running MMLU test : high_school_computer_science (count=100, batches=5) ...\n",
      "#### high_school_computer_science - accuracy=0.2300 , probability=0.2623\n",
      "### Running MMLU test : high_school_mathematics (count=270, batches=12) ...\n",
      "#### high_school_mathematics - accuracy=0.2630 , probability=0.2567\n",
      "### Running MMLU test : high_school_physics (count=151, batches=7) ...\n",
      "#### high_school_physics - accuracy=0.3311 , probability=0.3011\n",
      "### Running MMLU test : high_school_statistics (count=216, batches=9) ...\n",
      "#### high_school_statistics - accuracy=0.4722 , probability=0.3590\n",
      "### Running MMLU test : human_aging (count=223, batches=10) ...\n",
      "#### human_aging - accuracy=0.1614 , probability=0.2411\n",
      "### Running MMLU test : human_sexuality (count=131, batches=6) ...\n",
      "#### human_sexuality - accuracy=0.3359 , probability=0.3229\n",
      "### Running MMLU test : international_law (count=121, batches=6) ...\n",
      "#### international_law - accuracy=0.2314 , probability=0.2503\n",
      "### Running MMLU test : jurisprudence (count=108, batches=5) ...\n",
      "#### jurisprudence - accuracy=0.3056 , probability=0.3094\n",
      "### Running MMLU test : logical_fallacies (count=163, batches=7) ...\n",
      "#### logical_fallacies - accuracy=0.2761 , probability=0.2932\n",
      "### Running MMLU test : machine_learning (count=112, batches=5) ...\n",
      "#### machine_learning - accuracy=0.1875 , probability=0.2402\n",
      "### Running MMLU test : management (count=103, batches=5) ...\n",
      "#### management - accuracy=0.5146 , probability=0.4103\n",
      "### Running MMLU test : marketing (count=234, batches=10) ...\n",
      "#### marketing - accuracy=0.3718 , probability=0.3493\n",
      "### Running MMLU test : medical_genetics (count=100, batches=5) ...\n",
      "#### medical_genetics - accuracy=0.2900 , probability=0.2987\n",
      "### Running MMLU test : miscellaneous (count=783, batches=33) ...\n",
      "#### miscellaneous - accuracy=0.3857 , probability=0.3598\n",
      "### Running MMLU test : moral_disputes (count=346, batches=15) ...\n",
      "#### moral_disputes - accuracy=0.2370 , probability=0.2708\n",
      "### Running MMLU test : moral_scenarios (count=895, batches=38) ...\n",
      "#### moral_scenarios - accuracy=0.2737 , probability=0.2607\n",
      "### Running MMLU test : nutrition (count=306, batches=13) ...\n",
      "#### nutrition - accuracy=0.3301 , probability=0.3243\n",
      "### Running MMLU test : philosophy (count=311, batches=13) ...\n",
      "#### philosophy - accuracy=0.3215 , probability=0.3257\n",
      "### Running MMLU test : prehistory (count=324, batches=14) ...\n",
      "#### prehistory - accuracy=0.3056 , probability=0.3169\n",
      "### Running MMLU test : professional_law (count=1534, batches=64) ...\n",
      "#### professional_law - accuracy=0.2464 , probability=0.2572\n",
      "### Running MMLU test : professional_accounting (count=282, batches=12) ...\n",
      "#### professional_accounting - accuracy=0.2482 , probability=0.2574\n",
      "### Running MMLU test : professional_psychology (count=612, batches=26) ...\n",
      "#### professional_psychology - accuracy=0.2663 , probability=0.2793\n",
      "### Running MMLU test : professional_medicine (count=272, batches=12) ...\n",
      "#### professional_medicine - accuracy=0.4522 , probability=0.3623\n",
      "### Running MMLU test : public_relations (count=110, batches=5) ...\n",
      "#### public_relations - accuracy=0.3000 , probability=0.3039\n",
      "### Running MMLU test : security_studies (count=245, batches=11) ...\n",
      "#### security_studies - accuracy=0.4163 , probability=0.3511\n",
      "### Running MMLU test : sociology (count=201, batches=9) ...\n",
      "#### sociology - accuracy=0.3532 , probability=0.3613\n",
      "### Running MMLU test : us_foreign_policy (count=100, batches=5) ...\n",
      "#### us_foreign_policy - accuracy=0.3700 , probability=0.3406\n",
      "### Running MMLU test : virology (count=166, batches=7) ...\n",
      "#### virology - accuracy=0.2651 , probability=0.2900\n",
      "### Running MMLU test : world_religions (count=171, batches=8) ...\n",
      "#### world_religions - accuracy=0.5029 , probability=0.4285\n",
      "------------------------------------------------\n",
      "### MMLU overall test result : accuracy=0.3225 , probability=0.3094\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel (modified)\n",
    "# Batch size of 24, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 24 --n_shot 0 --tmix_backend \"triton_bighead\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-test-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : abstract_algebra (count=100, batches=5) ...\n",
      "#### abstract_algebra - accuracy=0.2100 , probability=0.2309\n",
      "### Running MMLU test : anatomy (count=135, batches=6) ...\n",
      "#### anatomy - accuracy=0.2963 , probability=0.3048\n",
      "### Running MMLU test : astronomy (count=152, batches=7) ...\n",
      "#### astronomy - accuracy=0.3882 , probability=0.3346\n",
      "### Running MMLU test : business_ethics (count=100, batches=5) ...\n",
      "#### business_ethics - accuracy=0.2800 , probability=0.2800\n",
      "### Running MMLU test : clinical_knowledge (count=265, batches=12) ...\n",
      "#### clinical_knowledge - accuracy=0.3962 , probability=0.3371\n",
      "### Running MMLU test : college_medicine (count=173, batches=8) ...\n",
      "#### college_medicine - accuracy=0.4104 , probability=0.3387\n",
      "### Running MMLU test : college_biology (count=144, batches=6) ...\n",
      "#### college_biology - accuracy=0.2917 , probability=0.3009\n",
      "### Running MMLU test : college_chemistry (count=100, batches=5) ...\n",
      "#### college_chemistry - accuracy=0.4100 , probability=0.3477\n",
      "### Running MMLU test : college_computer_science (count=100, batches=5) ...\n",
      "#### college_computer_science - accuracy=0.3300 , probability=0.3168\n",
      "### Running MMLU test : college_mathematics (count=100, batches=5) ...\n",
      "#### college_mathematics - accuracy=0.3000 , probability=0.2946\n",
      "### Running MMLU test : college_physics (count=102, batches=5) ...\n",
      "#### college_physics - accuracy=0.3824 , probability=0.3197\n",
      "### Running MMLU test : computer_security (count=100, batches=5) ...\n",
      "#### computer_security - accuracy=0.3100 , probability=0.3017\n",
      "### Running MMLU test : conceptual_physics (count=235, batches=10) ...\n",
      "#### conceptual_physics - accuracy=0.2298 , probability=0.2664\n",
      "### Running MMLU test : econometrics (count=114, batches=5) ...\n",
      "#### econometrics - accuracy=0.2456 , probability=0.2439\n",
      "### Running MMLU test : elementary_mathematics (count=378, batches=16) ...\n",
      "#### elementary_mathematics - accuracy=0.2672 , probability=0.2631\n",
      "### Running MMLU test : electrical_engineering (count=145, batches=7) ...\n",
      "#### electrical_engineering - accuracy=0.3103 , probability=0.2790\n",
      "### Running MMLU test : formal_logic (count=126, batches=6) ...\n",
      "#### formal_logic - accuracy=0.3730 , probability=0.2999\n",
      "### Running MMLU test : global_facts (count=100, batches=5) ...\n",
      "#### global_facts - accuracy=0.1800 , probability=0.2344\n",
      "### Running MMLU test : high_school_european_history (count=165, batches=7) ...\n",
      "#### high_school_european_history - accuracy=0.2848 , probability=0.3124\n",
      "### Running MMLU test : high_school_us_history (count=204, batches=9) ...\n",
      "#### high_school_us_history - accuracy=0.2941 , probability=0.3124\n",
      "### Running MMLU test : high_school_world_history (count=237, batches=10) ...\n",
      "#### high_school_world_history - accuracy=0.2152 , probability=0.2737\n",
      "### Running MMLU test : high_school_geography (count=198, batches=9) ...\n",
      "#### high_school_geography - accuracy=0.4444 , probability=0.3845\n",
      "### Running MMLU test : high_school_government_and_politics (count=193, batches=9) ...\n",
      "#### high_school_government_and_politics - accuracy=0.4249 , probability=0.3960\n",
      "### Running MMLU test : high_school_macroeconomics (count=390, batches=17) ...\n",
      "#### high_school_macroeconomics - accuracy=0.3846 , probability=0.3391\n",
      "### Running MMLU test : high_school_microeconomics (count=238, batches=10) ...\n",
      "#### high_school_microeconomics - accuracy=0.3697 , probability=0.3374\n",
      "### Running MMLU test : high_school_psychology (count=545, batches=23) ...\n",
      "#### high_school_psychology - accuracy=0.4697 , probability=0.3951\n",
      "### Running MMLU test : high_school_biology (count=310, batches=13) ...\n",
      "#### high_school_biology - accuracy=0.3548 , probability=0.3390\n",
      "### Running MMLU test : high_school_chemistry (count=203, batches=9) ...\n",
      "#### high_school_chemistry - accuracy=0.2906 , probability=0.2687\n",
      "### Running MMLU test : high_school_computer_science (count=100, batches=5) ...\n",
      "#### high_school_computer_science - accuracy=0.2300 , probability=0.2622\n",
      "### Running MMLU test : high_school_mathematics (count=270, batches=12) ...\n",
      "#### high_school_mathematics - accuracy=0.2630 , probability=0.2565\n",
      "### Running MMLU test : high_school_physics (count=151, batches=7) ...\n",
      "#### high_school_physics - accuracy=0.3311 , probability=0.3014\n",
      "### Running MMLU test : high_school_statistics (count=216, batches=9) ...\n",
      "#### high_school_statistics - accuracy=0.4722 , probability=0.3592\n",
      "### Running MMLU test : human_aging (count=223, batches=10) ...\n",
      "#### human_aging - accuracy=0.1614 , probability=0.2408\n",
      "### Running MMLU test : human_sexuality (count=131, batches=6) ...\n",
      "#### human_sexuality - accuracy=0.3359 , probability=0.3235\n",
      "### Running MMLU test : international_law (count=121, batches=6) ...\n",
      "#### international_law - accuracy=0.2314 , probability=0.2506\n",
      "### Running MMLU test : jurisprudence (count=108, batches=5) ...\n",
      "#### jurisprudence - accuracy=0.3056 , probability=0.3100\n",
      "### Running MMLU test : logical_fallacies (count=163, batches=7) ...\n",
      "#### logical_fallacies - accuracy=0.2761 , probability=0.2922\n",
      "### Running MMLU test : machine_learning (count=112, batches=5) ...\n",
      "#### machine_learning - accuracy=0.1786 , probability=0.2410\n",
      "### Running MMLU test : management (count=103, batches=5) ...\n",
      "#### management - accuracy=0.5243 , probability=0.4113\n",
      "### Running MMLU test : marketing (count=234, batches=10) ...\n",
      "#### marketing - accuracy=0.3803 , probability=0.3493\n",
      "### Running MMLU test : medical_genetics (count=100, batches=5) ...\n",
      "#### medical_genetics - accuracy=0.3100 , probability=0.2998\n",
      "### Running MMLU test : miscellaneous (count=783, batches=33) ...\n",
      "#### miscellaneous - accuracy=0.3883 , probability=0.3599\n",
      "### Running MMLU test : moral_disputes (count=346, batches=15) ...\n",
      "#### moral_disputes - accuracy=0.2341 , probability=0.2702\n",
      "### Running MMLU test : moral_scenarios (count=895, batches=38) ...\n",
      "#### moral_scenarios - accuracy=0.2737 , probability=0.2603\n",
      "### Running MMLU test : nutrition (count=306, batches=13) ...\n",
      "#### nutrition - accuracy=0.3333 , probability=0.3250\n",
      "### Running MMLU test : philosophy (count=311, batches=13) ...\n",
      "#### philosophy - accuracy=0.3215 , probability=0.3252\n",
      "### Running MMLU test : prehistory (count=324, batches=14) ...\n",
      "#### prehistory - accuracy=0.3056 , probability=0.3172\n",
      "### Running MMLU test : professional_law (count=1534, batches=64) ...\n",
      "#### professional_law - accuracy=0.2458 , probability=0.2575\n",
      "### Running MMLU test : professional_accounting (count=282, batches=12) ...\n",
      "#### professional_accounting - accuracy=0.2482 , probability=0.2567\n",
      "### Running MMLU test : professional_psychology (count=612, batches=26) ...\n",
      "#### professional_psychology - accuracy=0.2680 , probability=0.2792\n",
      "### Running MMLU test : professional_medicine (count=272, batches=12) ...\n",
      "#### professional_medicine - accuracy=0.4522 , probability=0.3616\n",
      "### Running MMLU test : public_relations (count=110, batches=5) ...\n",
      "#### public_relations - accuracy=0.3000 , probability=0.3042\n",
      "### Running MMLU test : security_studies (count=245, batches=11) ...\n",
      "#### security_studies - accuracy=0.4163 , probability=0.3503\n",
      "### Running MMLU test : sociology (count=201, batches=9) ...\n",
      "#### sociology - accuracy=0.3632 , probability=0.3613\n",
      "### Running MMLU test : us_foreign_policy (count=100, batches=5) ...\n",
      "#### us_foreign_policy - accuracy=0.3700 , probability=0.3402\n",
      "### Running MMLU test : virology (count=166, batches=7) ...\n",
      "#### virology - accuracy=0.2590 , probability=0.2895\n",
      "### Running MMLU test : world_religions (count=171, batches=8) ...\n",
      "#### world_religions - accuracy=0.5205 , probability=0.4287\n",
      "------------------------------------------------\n",
      "### MMLU overall test result : accuracy=0.3236 , probability=0.3094\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel (modified)\n",
    "# Batch size of 24, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 24 --n_shot 0 --tmix_backend \"fla\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-test-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : abstract_algebra (count=100, batches=5) ...\n",
      "#### abstract_algebra - accuracy=0.2100 , probability=0.2309\n",
      "### Running MMLU test : anatomy (count=135, batches=6) ...\n",
      "#### anatomy - accuracy=0.2963 , probability=0.3040\n",
      "### Running MMLU test : astronomy (count=152, batches=7) ...\n",
      "#### astronomy - accuracy=0.3882 , probability=0.3345\n",
      "### Running MMLU test : business_ethics (count=100, batches=5) ...\n",
      "#### business_ethics - accuracy=0.2800 , probability=0.2811\n",
      "### Running MMLU test : clinical_knowledge (count=265, batches=12) ...\n",
      "#### clinical_knowledge - accuracy=0.3962 , probability=0.3369\n",
      "### Running MMLU test : college_medicine (count=173, batches=8) ...\n",
      "#### college_medicine - accuracy=0.4046 , probability=0.3385\n",
      "### Running MMLU test : college_biology (count=144, batches=6) ...\n",
      "#### college_biology - accuracy=0.2917 , probability=0.3011\n",
      "### Running MMLU test : college_chemistry (count=100, batches=5) ...\n",
      "#### college_chemistry - accuracy=0.4100 , probability=0.3474\n",
      "### Running MMLU test : college_computer_science (count=100, batches=5) ...\n",
      "#### college_computer_science - accuracy=0.3300 , probability=0.3165\n",
      "### Running MMLU test : college_mathematics (count=100, batches=5) ...\n",
      "#### college_mathematics - accuracy=0.3000 , probability=0.2955\n",
      "### Running MMLU test : college_physics (count=102, batches=5) ...\n",
      "#### college_physics - accuracy=0.3824 , probability=0.3195\n",
      "### Running MMLU test : computer_security (count=100, batches=5) ...\n",
      "#### computer_security - accuracy=0.3200 , probability=0.3028\n",
      "### Running MMLU test : conceptual_physics (count=235, batches=10) ...\n",
      "#### conceptual_physics - accuracy=0.2298 , probability=0.2659\n",
      "### Running MMLU test : econometrics (count=114, batches=5) ...\n",
      "#### econometrics - accuracy=0.2456 , probability=0.2442\n",
      "### Running MMLU test : elementary_mathematics (count=378, batches=16) ...\n",
      "#### elementary_mathematics - accuracy=0.2672 , probability=0.2629\n",
      "### Running MMLU test : electrical_engineering (count=145, batches=7) ...\n",
      "#### electrical_engineering - accuracy=0.3034 , probability=0.2791\n",
      "### Running MMLU test : formal_logic (count=126, batches=6) ...\n",
      "#### formal_logic - accuracy=0.3730 , probability=0.2996\n",
      "### Running MMLU test : global_facts (count=100, batches=5) ...\n",
      "#### global_facts - accuracy=0.1800 , probability=0.2341\n",
      "### Running MMLU test : high_school_european_history (count=165, batches=7) ...\n",
      "#### high_school_european_history - accuracy=0.2848 , probability=0.3134\n",
      "### Running MMLU test : high_school_us_history (count=204, batches=9) ...\n",
      "#### high_school_us_history - accuracy=0.2941 , probability=0.3127\n",
      "### Running MMLU test : high_school_world_history (count=237, batches=10) ...\n",
      "#### high_school_world_history - accuracy=0.2236 , probability=0.2741\n",
      "### Running MMLU test : high_school_geography (count=198, batches=9) ...\n",
      "#### high_school_geography - accuracy=0.4444 , probability=0.3840\n",
      "### Running MMLU test : high_school_government_and_politics (count=193, batches=9) ...\n",
      "#### high_school_government_and_politics - accuracy=0.4197 , probability=0.3961\n",
      "### Running MMLU test : high_school_macroeconomics (count=390, batches=17) ...\n",
      "#### high_school_macroeconomics - accuracy=0.3846 , probability=0.3393\n",
      "### Running MMLU test : high_school_microeconomics (count=238, batches=10) ...\n",
      "#### high_school_microeconomics - accuracy=0.3697 , probability=0.3381\n",
      "### Running MMLU test : high_school_psychology (count=545, batches=23) ...\n",
      "#### high_school_psychology - accuracy=0.4679 , probability=0.3950\n",
      "### Running MMLU test : high_school_biology (count=310, batches=13) ...\n",
      "#### high_school_biology - accuracy=0.3548 , probability=0.3392\n",
      "### Running MMLU test : high_school_chemistry (count=203, batches=9) ...\n",
      "#### high_school_chemistry - accuracy=0.2906 , probability=0.2687\n",
      "### Running MMLU test : high_school_computer_science (count=100, batches=5) ...\n",
      "#### high_school_computer_science - accuracy=0.2300 , probability=0.2626\n",
      "### Running MMLU test : high_school_mathematics (count=270, batches=12) ...\n",
      "#### high_school_mathematics - accuracy=0.2630 , probability=0.2561\n",
      "### Running MMLU test : high_school_physics (count=151, batches=7) ...\n",
      "#### high_school_physics - accuracy=0.3311 , probability=0.3007\n",
      "### Running MMLU test : high_school_statistics (count=216, batches=9) ...\n",
      "#### high_school_statistics - accuracy=0.4722 , probability=0.3586\n",
      "### Running MMLU test : human_aging (count=223, batches=10) ...\n",
      "#### human_aging - accuracy=0.1614 , probability=0.2406\n",
      "### Running MMLU test : human_sexuality (count=131, batches=6) ...\n",
      "#### human_sexuality - accuracy=0.3359 , probability=0.3229\n",
      "### Running MMLU test : international_law (count=121, batches=6) ...\n",
      "#### international_law - accuracy=0.2231 , probability=0.2506\n",
      "### Running MMLU test : jurisprudence (count=108, batches=5) ...\n",
      "#### jurisprudence - accuracy=0.3056 , probability=0.3095\n",
      "### Running MMLU test : logical_fallacies (count=163, batches=7) ...\n",
      "#### logical_fallacies - accuracy=0.2761 , probability=0.2933\n",
      "### Running MMLU test : machine_learning (count=112, batches=5) ...\n",
      "#### machine_learning - accuracy=0.1875 , probability=0.2401\n",
      "### Running MMLU test : management (count=103, batches=5) ...\n",
      "#### management - accuracy=0.5243 , probability=0.4103\n",
      "### Running MMLU test : marketing (count=234, batches=10) ...\n",
      "#### marketing - accuracy=0.3718 , probability=0.3496\n",
      "### Running MMLU test : medical_genetics (count=100, batches=5) ...\n",
      "#### medical_genetics - accuracy=0.2900 , probability=0.2985\n",
      "### Running MMLU test : miscellaneous (count=783, batches=33) ...\n",
      "#### miscellaneous - accuracy=0.3883 , probability=0.3598\n",
      "### Running MMLU test : moral_disputes (count=346, batches=15) ...\n",
      "#### moral_disputes - accuracy=0.2370 , probability=0.2706\n",
      "### Running MMLU test : moral_scenarios (count=895, batches=38) ...\n",
      "#### moral_scenarios - accuracy=0.2737 , probability=0.2608\n",
      "### Running MMLU test : nutrition (count=306, batches=13) ...\n",
      "#### nutrition - accuracy=0.3268 , probability=0.3246\n",
      "### Running MMLU test : philosophy (count=311, batches=13) ...\n",
      "#### philosophy - accuracy=0.3280 , probability=0.3254\n",
      "### Running MMLU test : prehistory (count=324, batches=14) ...\n",
      "#### prehistory - accuracy=0.3086 , probability=0.3171\n",
      "### Running MMLU test : professional_law (count=1534, batches=64) ...\n",
      "#### professional_law - accuracy=0.2458 , probability=0.2574\n",
      "### Running MMLU test : professional_accounting (count=282, batches=12) ...\n",
      "#### professional_accounting - accuracy=0.2482 , probability=0.2576\n",
      "### Running MMLU test : professional_psychology (count=612, batches=26) ...\n",
      "#### professional_psychology - accuracy=0.2647 , probability=0.2791\n",
      "### Running MMLU test : professional_medicine (count=272, batches=12) ...\n",
      "#### professional_medicine - accuracy=0.4522 , probability=0.3623\n",
      "### Running MMLU test : public_relations (count=110, batches=5) ...\n",
      "#### public_relations - accuracy=0.3000 , probability=0.3040\n",
      "### Running MMLU test : security_studies (count=245, batches=11) ...\n",
      "#### security_studies - accuracy=0.4204 , probability=0.3512\n",
      "### Running MMLU test : sociology (count=201, batches=9) ...\n",
      "#### sociology - accuracy=0.3632 , probability=0.3610\n",
      "### Running MMLU test : us_foreign_policy (count=100, batches=5) ...\n",
      "#### us_foreign_policy - accuracy=0.3700 , probability=0.3401\n",
      "### Running MMLU test : virology (count=166, batches=7) ...\n",
      "#### virology - accuracy=0.2651 , probability=0.2899\n",
      "### Running MMLU test : world_religions (count=171, batches=8) ...\n",
      "#### world_religions - accuracy=0.5088 , probability=0.4286\n",
      "------------------------------------------------\n",
      "### MMLU overall test result : accuracy=0.3231 , probability=0.3094\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel (modified)\n",
    "# Batch size of 32, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 24 --n_shot 0 --tmix_backend \"fla_fused\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py-3-12]",
   "language": "python",
   "name": "conda-env-py-3-12-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
