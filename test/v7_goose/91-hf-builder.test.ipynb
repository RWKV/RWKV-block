{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face builder test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/v7_goose\n",
      "Model file path: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/v7_goose/.model/v7-1B5-world.pth\n",
      "Project directory: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block\n",
      "Output directory: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/v7_goose/.hf_build/v7-1B5-world/\n"
     ]
    }
   ],
   "source": [
    "# Get the current script directory, from the notebook\n",
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "print(\"Notebook directory:\", notebook_dir)\n",
    "\n",
    "model_filename = \"v7-1B5-world\"\n",
    "model_file = os.path.join(notebook_dir, \".model\", f\"{model_filename}.pth\")\n",
    "print(\"Model file path:\", model_file)\n",
    "\n",
    "# Check if the model file exists\n",
    "if os.path.isfile(model_file) is False:\n",
    "    raise Exception(\"Model file does not exist\")\n",
    "\n",
    "# Get the project directory two levels up\n",
    "project_dir = os.path.dirname(os.path.dirname(notebook_dir))\n",
    "print(\"Project directory:\", project_dir)\n",
    "\n",
    "# Output build directory\n",
    "output_dir = os.path.join(notebook_dir, f\".hf_build/{model_filename}/\")\n",
    "print(\"Output directory:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Converting RWKV model to HuggingFace format...\n",
      "Model Class     : v7_goose\n",
      "Model Source    : /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/v7_goose/.model/v7-1B5-world.pth\n",
      "Tokenizer Type  : auto\n",
      "Output Directory: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "-----------------------------\n",
      "Building rwkv_block into HF code ...\n",
      "Loading model weights raw state ...\n",
      "Loading model config from weights ...\n",
      "-----------------------------\n",
      "Model Configuration:\n",
      "{'vocab_size': 65536, 'num_hidden_layers': 24, 'hidden_size': 2048, 'hidden_size_att': 2048, 'hidden_size_ffn': 8192, 'head_size': 64, 'tmix_backend': 'auto', 'init_state_wkv': False, 'forward_chunk_size': 4096, 'dropout_rate': 0.0, 'use_cache': True, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': False, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': None, 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 0, 'pad_token_id': None, 'eos_token_id': 0, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '', '_commit_hash': None, '_attn_implementation_internal': None, '_attn_implementation_autoset': False, 'transformers_version': None, 'layer_id': None, 'device': None, 'dtype': None}\n",
      "-----------------------------\n",
      "Loading model class instance ...\n",
      "Detected Tokenizer Type: world\n",
      "Loading model state into class ...\n",
      "-----------------------------\n",
      "Saving tokenizer files ...\n",
      "Saving model code files ...\n",
      "Saving model weight files ...\n",
      "Patching configuration ...\n",
      "-----------------------------\n",
      "Successfully converted RWKV model to HuggingFace format\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Empty the output directory, if it exists\n",
    "if os.path.isdir(output_dir):\n",
    "    import shutil\n",
    "    print(\"Removing existing output directory\")\n",
    "    shutil.rmtree(output_dir)\n",
    "    \n",
    "# Run the hf_builder.py\n",
    "!python3 \"$project_dir/hf_builder/hf_builder.py\" --model_class \"v7_goose\" \"$model_file\" \"$output_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic HELLO WORLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully\n",
      "Running on device: cuda\n",
      "---------------------------------\n",
      "Prompt: HELLO WORLD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: HELLO WORLD!\n",
      "I am a newbie to this forum. I am trying to learn how to use the\n",
      "---------------------------------\n",
      "Prompt: \n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\n",
      "Generated text: \n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\n",
      "The dragons were discovered by a team of scientists led by Dr. John Smith, who was studying\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the built model, using the transformers library\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "# Validating the config and tokenizer are built correctly\n",
    "config = AutoConfig.from_pretrained(output_dir, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir, trust_remote_code=True)\n",
    "\n",
    "# Move the model to the GPU\n",
    "RUN_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Build the model itself\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir, trust_remote_code=True, tmix_backend=\"triton\", device=RUN_DEVICE)\n",
    "model.to(RUN_DEVICE)\n",
    "print(\"Model and tokenizer loaded successfully\")\n",
    "\n",
    "# Print the device being used\n",
    "print(\"Running on device:\", RUN_DEVICE)\n",
    "\n",
    "# Lets generate some text, using the model on the GPU\n",
    "dragon_prompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n",
    "hellow_prompt = \"HELLO WORLD\"\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Prompt: {hellow_prompt}\")\n",
    "inputs = tokenizer(hellow_prompt, return_tensors=\"pt\").to(RUN_DEVICE)\n",
    "outputs = model.generate(**inputs)\n",
    "print(\"Generated text:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Prompt: {dragon_prompt}\")\n",
    "inputs = tokenizer(dragon_prompt, return_tensors=\"pt\").to(RUN_DEVICE)\n",
    "outputs = model.generate(**inputs)\n",
    "print(\"Generated text:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU validation testing (smaller set)\n",
    "**(this is not a substitute for lm-eval-harness : the score is counted differently)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Using HF model tokenizer: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "## Building MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/mmlu/.mmlu_cache/mmlu-val-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Building dataset for validation subject (n_shot=0): all\n",
      "## Dataset is ready for validation subject (n_shot=0): all\n",
      "## Longest prompt token length: 781\n",
      "## Padding to target prompt length: 784\n",
      "## Dataset is padded for validation subject (n_shot=0): all\n",
      "## Saving MMLU dataset cache (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/mmlu/.mmlu_cache/mmlu-val-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n"
     ]
    }
   ],
   "source": [
    "# MMLU tester directory\n",
    "mmlu_test_dir = os.path.join(project_dir, \"test/mmlu\")\n",
    "\n",
    "# Run the test dataset builder, optional:  --use_validation_set\n",
    "!python3 {mmlu_test_dir}/BuildTestMMLU.py --hf_model \"$output_dir\" --n_shot 0 --use_validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/mmlu/.mmlu_cache/mmlu-val-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : all (count=1531, batches=48) ...\n",
      "Using /home/recursal/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py312_cu121/state_wind_backstepping/build.ninja...\n",
      "/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module state_wind_backstepping...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module state_wind_backstepping...\n",
      "#### all - accuracy=0.3063 , probability=0.2982\n",
      "------------------------------------------------\n",
      "### MMLU overall test result : accuracy=0.3063 , probability=0.2982\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the cuda kernel\n",
    "# Batch size of 32, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 32 --n_shot 0 --use_validation_set --tmix_backend \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/mmlu/.mmlu_cache/mmlu-val-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : all (count=1531, batches=48) ...\n",
      "#### all - accuracy=0.3050 , probability=0.2982\n",
      "------------------------------------------------\n",
      "### MMLU overall test result : accuracy=0.3050 , probability=0.2982\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel\n",
    "# Batch size of 32, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 32 --n_shot 0 --use_validation_set --tmix_backend \"triton\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/mmlu/.mmlu_cache/mmlu-val-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : all (count=1531, batches=48) ...\n",
      "#### all - accuracy=0.3031 , probability=0.2981\n",
      "------------------------------------------------\n",
      "### MMLU overall test result : accuracy=0.3031 , probability=0.2981\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel\n",
    "# Batch size of 32, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 32 --n_shot 0 --use_validation_set --tmix_backend \"fla\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU testing \n",
    "**(this is not a substitute for lm-eval-harness : the score is counted differently)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/mmlu/.mmlu_cache/mmlu-test-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : abstract_algebra (count=100, batches=4) ...\n",
      "Using /home/recursal/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py312_cu121/state_wind_backstepping/build.ninja...\n",
      "/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module state_wind_backstepping...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module state_wind_backstepping...\n",
      "#### abstract_algebra - accuracy=0.2100 , probability=0.2313\n",
      "### Running MMLU test : anatomy (count=135, batches=5) ...\n",
      "#### anatomy - accuracy=0.2963 , probability=0.3041\n",
      "### Running MMLU test : astronomy (count=152, batches=5) ...\n",
      "#### astronomy - accuracy=0.3882 , probability=0.3342\n",
      "### Running MMLU test : business_ethics (count=100, batches=4) ...\n",
      "#### business_ethics - accuracy=0.2800 , probability=0.2795\n",
      "### Running MMLU test : clinical_knowledge (count=265, batches=9) ...\n",
      "#### clinical_knowledge - accuracy=0.3962 , probability=0.3373\n",
      "### Running MMLU test : college_medicine (count=173, batches=6) ...\n",
      "#### college_medicine - accuracy=0.4104 , probability=0.3390\n",
      "### Running MMLU test : college_biology (count=144, batches=5) ...\n",
      "#### college_biology - accuracy=0.2986 , probability=0.3015\n",
      "### Running MMLU test : college_chemistry (count=100, batches=4) ...\n",
      "#### college_chemistry - accuracy=0.4100 , probability=0.3474\n",
      "### Running MMLU test : college_computer_science (count=100, batches=4) ...\n",
      "#### college_computer_science - accuracy=0.3300 , probability=0.3167\n",
      "### Running MMLU test : college_mathematics (count=100, batches=4) ...\n",
      "#### college_mathematics - accuracy=0.3000 , probability=0.2948\n",
      "### Running MMLU test : college_physics (count=102, batches=4) ...\n",
      "#### college_physics - accuracy=0.3824 , probability=0.3190\n",
      "### Running MMLU test : computer_security (count=100, batches=4) ...\n",
      "#### computer_security - accuracy=0.3100 , probability=0.3019\n",
      "### Running MMLU test : conceptual_physics (count=235, batches=8) ...\n",
      "#### conceptual_physics - accuracy=0.2340 , probability=0.2669\n",
      "### Running MMLU test : econometrics (count=114, batches=4) ...\n",
      "#### econometrics - accuracy=0.2456 , probability=0.2439\n",
      "### Running MMLU test : elementary_mathematics (count=378, batches=12) ...\n",
      "#### elementary_mathematics - accuracy=0.2672 , probability=0.2626\n",
      "### Running MMLU test : electrical_engineering (count=145, batches=5) ...\n",
      "#### electrical_engineering - accuracy=0.2966 , probability=0.2785\n",
      "### Running MMLU test : formal_logic (count=126, batches=4) ...\n",
      "#### formal_logic - accuracy=0.3730 , probability=0.3001\n",
      "### Running MMLU test : global_facts (count=100, batches=4) ...\n",
      "#### global_facts - accuracy=0.1800 , probability=0.2330\n",
      "### Running MMLU test : high_school_european_history (count=165, batches=6) ...\n",
      "#### high_school_european_history - accuracy=0.2848 , probability=0.3137\n",
      "### Running MMLU test : high_school_us_history (count=204, batches=7) ...\n",
      "#### high_school_us_history - accuracy=0.2990 , probability=0.3137\n",
      "### Running MMLU test : high_school_world_history (count=237, batches=8) ...\n",
      "#### high_school_world_history - accuracy=0.2278 , probability=0.2745\n",
      "### Running MMLU test : high_school_geography (count=198, batches=7) ...\n",
      "#### high_school_geography - accuracy=0.4444 , probability=0.3846\n",
      "### Running MMLU test : high_school_government_and_politics (count=193, batches=7) ...\n",
      "#### high_school_government_and_politics - accuracy=0.4197 , probability=0.3963\n",
      "### Running MMLU test : high_school_macroeconomics (count=390, batches=13) ...\n",
      "#### high_school_macroeconomics - accuracy=0.3846 , probability=0.3393\n",
      "### Running MMLU test : high_school_microeconomics (count=238, batches=8) ...\n",
      "#### high_school_microeconomics - accuracy=0.3697 , probability=0.3373\n",
      "### Running MMLU test : high_school_psychology (count=545, batches=18) ...\n",
      "#### high_school_psychology - accuracy=0.4606 , probability=0.3954\n",
      "### Running MMLU test : high_school_biology (count=310, batches=10) ...\n",
      "#### high_school_biology - accuracy=0.3548 , probability=0.3398\n",
      "### Running MMLU test : high_school_chemistry (count=203, batches=7) ...\n",
      "#### high_school_chemistry - accuracy=0.2906 , probability=0.2691\n",
      "### Running MMLU test : high_school_computer_science (count=100, batches=4) ...\n",
      "#### high_school_computer_science - accuracy=0.2300 , probability=0.2625\n",
      "### Running MMLU test : high_school_mathematics (count=270, batches=9) ...\n",
      "#### high_school_mathematics - accuracy=0.2630 , probability=0.2559\n",
      "### Running MMLU test : high_school_physics (count=151, batches=5) ...\n",
      "#### high_school_physics - accuracy=0.3311 , probability=0.3005\n",
      "### Running MMLU test : high_school_statistics (count=216, batches=7) ...\n",
      "#### high_school_statistics - accuracy=0.4722 , probability=0.3585\n",
      "### Running MMLU test : human_aging (count=223, batches=7) ...\n",
      "#### human_aging - accuracy=0.1614 , probability=0.2412\n",
      "### Running MMLU test : human_sexuality (count=131, batches=5) ...\n",
      "#### human_sexuality - accuracy=0.3359 , probability=0.3231\n",
      "### Running MMLU test : international_law (count=121, batches=4) ...\n",
      "#### international_law - accuracy=0.2314 , probability=0.2516\n",
      "### Running MMLU test : jurisprudence (count=108, batches=4) ...\n",
      "#### jurisprudence - accuracy=0.3056 , probability=0.3100\n",
      "### Running MMLU test : logical_fallacies (count=163, batches=6) ...\n",
      "#### logical_fallacies - accuracy=0.2761 , probability=0.2937\n",
      "### Running MMLU test : machine_learning (count=112, batches=4) ...\n",
      "#### machine_learning - accuracy=0.1964 , probability=0.2411\n",
      "### Running MMLU test : management (count=103, batches=4) ...\n",
      "#### management - accuracy=0.5243 , probability=0.4101\n",
      "### Running MMLU test : marketing (count=234, batches=8) ...\n",
      "#### marketing - accuracy=0.3718 , probability=0.3497\n",
      "### Running MMLU test : medical_genetics (count=100, batches=4) ...\n",
      "#### medical_genetics - accuracy=0.2900 , probability=0.2983\n",
      "### Running MMLU test : miscellaneous (count=783, batches=25) ...\n",
      "#### miscellaneous - accuracy=0.3857 , probability=0.3597\n",
      "### Running MMLU test : moral_disputes (count=346, batches=11) ...\n",
      "#### moral_disputes - accuracy=0.2341 , probability=0.2709\n",
      "### Running MMLU test : moral_scenarios (count=895, batches=28) ...\n",
      "#### moral_scenarios - accuracy=0.2737 , probability=0.2605\n",
      "### Running MMLU test : nutrition (count=306, batches=10) ...\n",
      "#### nutrition - accuracy=0.3301 , probability=0.3251\n",
      "### Running MMLU test : philosophy (count=311, batches=10) ...\n",
      "#### philosophy - accuracy=0.3280 , probability=0.3258\n",
      "### Running MMLU test : prehistory (count=324, batches=11) ...\n",
      "#### prehistory - accuracy=0.3025 , probability=0.3171\n",
      "### Running MMLU test : professional_law (count=1534, batches=48) ...\n",
      "#### professional_law - accuracy=0.2464 , probability=0.2575\n",
      "### Running MMLU test : professional_accounting (count=282, batches=9) ...\n",
      "#### professional_accounting - accuracy=0.2482 , probability=0.2572\n",
      "### Running MMLU test : professional_psychology (count=612, batches=20) ...\n",
      "#### professional_psychology - accuracy=0.2696 , probability=0.2795\n",
      "### Running MMLU test : professional_medicine (count=272, batches=9) ...\n",
      "#### professional_medicine - accuracy=0.4522 , probability=0.3620\n",
      "### Running MMLU test : public_relations (count=110, batches=4) ...\n",
      "#### public_relations - accuracy=0.3000 , probability=0.3042\n",
      "### Running MMLU test : security_studies (count=245, batches=8) ...\n",
      "#### security_studies - accuracy=0.4163 , probability=0.3509\n",
      "### Running MMLU test : sociology (count=201, batches=7) ...\n",
      "#### sociology - accuracy=0.3532 , probability=0.3607\n",
      "### Running MMLU test : us_foreign_policy (count=100, batches=4) ...\n",
      "#### us_foreign_policy - accuracy=0.3700 , probability=0.3413\n",
      "### Running MMLU test : virology (count=166, batches=6) ...\n",
      "#### virology - accuracy=0.2590 , probability=0.2899\n",
      "### Running MMLU test : world_religions (count=171, batches=6) ...\n",
      "#### world_religions - accuracy=0.5146 , probability=0.4285\n",
      "------------------------------------------------\n",
      "### MMLU overall test result : accuracy=0.3231 , probability=0.3095\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the cuda kernel\n",
    "# Batch size of 32, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 32 --n_shot 0 --tmix_backend \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/mmlu/.mmlu_cache/mmlu-test-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : abstract_algebra (count=100, batches=4) ...\n",
      "#### abstract_algebra - accuracy=0.2100 , probability=0.2314\n",
      "### Running MMLU test : anatomy (count=135, batches=5) ...\n",
      "#### anatomy - accuracy=0.2963 , probability=0.3043\n",
      "### Running MMLU test : astronomy (count=152, batches=5) ...\n",
      "#### astronomy - accuracy=0.3882 , probability=0.3352\n",
      "### Running MMLU test : business_ethics (count=100, batches=4) ...\n",
      "#### business_ethics - accuracy=0.2800 , probability=0.2814\n",
      "### Running MMLU test : clinical_knowledge (count=265, batches=9) ...\n",
      "#### clinical_knowledge - accuracy=0.3925 , probability=0.3374\n",
      "### Running MMLU test : college_medicine (count=173, batches=6) ...\n",
      "#### college_medicine - accuracy=0.4104 , probability=0.3390\n",
      "### Running MMLU test : college_biology (count=144, batches=5) ...\n",
      "#### college_biology - accuracy=0.2917 , probability=0.3010\n",
      "### Running MMLU test : college_chemistry (count=100, batches=4) ...\n",
      "#### college_chemistry - accuracy=0.4100 , probability=0.3478\n",
      "### Running MMLU test : college_computer_science (count=100, batches=4) ...\n",
      "#### college_computer_science - accuracy=0.3300 , probability=0.3167\n",
      "### Running MMLU test : college_mathematics (count=100, batches=4) ...\n",
      "#### college_mathematics - accuracy=0.3000 , probability=0.2956\n",
      "### Running MMLU test : college_physics (count=102, batches=4) ...\n",
      "#### college_physics - accuracy=0.3824 , probability=0.3192\n",
      "### Running MMLU test : computer_security (count=100, batches=4) ...\n",
      "#### computer_security - accuracy=0.3300 , probability=0.3027\n",
      "### Running MMLU test : conceptual_physics (count=235, batches=8) ...\n",
      "#### conceptual_physics - accuracy=0.2298 , probability=0.2661\n",
      "### Running MMLU test : econometrics (count=114, batches=4) ...\n",
      "#### econometrics - accuracy=0.2456 , probability=0.2441\n",
      "### Running MMLU test : elementary_mathematics (count=378, batches=12) ...\n",
      "#### elementary_mathematics - accuracy=0.2672 , probability=0.2629\n",
      "### Running MMLU test : electrical_engineering (count=145, batches=5) ...\n",
      "#### electrical_engineering - accuracy=0.2966 , probability=0.2786\n",
      "### Running MMLU test : formal_logic (count=126, batches=4) ...\n",
      "#### formal_logic - accuracy=0.3730 , probability=0.2992\n",
      "### Running MMLU test : global_facts (count=100, batches=4) ...\n",
      "#### global_facts - accuracy=0.1800 , probability=0.2336\n",
      "### Running MMLU test : high_school_european_history (count=165, batches=6) ...\n",
      "#### high_school_european_history - accuracy=0.2788 , probability=0.3131\n",
      "### Running MMLU test : high_school_us_history (count=204, batches=7) ...\n",
      "#### high_school_us_history - accuracy=0.2941 , probability=0.3128\n",
      "### Running MMLU test : high_school_world_history (count=237, batches=8) ...\n",
      "#### high_school_world_history - accuracy=0.2278 , probability=0.2742\n",
      "### Running MMLU test : high_school_geography (count=198, batches=7) ...\n",
      "#### high_school_geography - accuracy=0.4394 , probability=0.3847\n",
      "### Running MMLU test : high_school_government_and_politics (count=193, batches=7) ...\n",
      "#### high_school_government_and_politics - accuracy=0.4301 , probability=0.3964\n",
      "### Running MMLU test : high_school_macroeconomics (count=390, batches=13) ...\n",
      "#### high_school_macroeconomics - accuracy=0.3846 , probability=0.3395\n",
      "### Running MMLU test : high_school_microeconomics (count=238, batches=8) ...\n",
      "#### high_school_microeconomics - accuracy=0.3697 , probability=0.3375\n",
      "### Running MMLU test : high_school_psychology (count=545, batches=18) ...\n",
      "#### high_school_psychology - accuracy=0.4642 , probability=0.3951\n",
      "### Running MMLU test : high_school_biology (count=310, batches=10) ...\n",
      "#### high_school_biology - accuracy=0.3548 , probability=0.3390\n",
      "### Running MMLU test : high_school_chemistry (count=203, batches=7) ...\n",
      "#### high_school_chemistry - accuracy=0.2906 , probability=0.2685\n",
      "### Running MMLU test : high_school_computer_science (count=100, batches=4) ...\n",
      "#### high_school_computer_science - accuracy=0.2300 , probability=0.2625\n",
      "### Running MMLU test : high_school_mathematics (count=270, batches=9) ...\n",
      "#### high_school_mathematics - accuracy=0.2630 , probability=0.2561\n",
      "### Running MMLU test : high_school_physics (count=151, batches=5) ...\n",
      "#### high_school_physics - accuracy=0.3311 , probability=0.3012\n",
      "### Running MMLU test : high_school_statistics (count=216, batches=7) ...\n",
      "#### high_school_statistics - accuracy=0.4722 , probability=0.3588\n",
      "### Running MMLU test : human_aging (count=223, batches=7) ...\n",
      "#### human_aging - accuracy=0.1614 , probability=0.2405\n",
      "### Running MMLU test : human_sexuality (count=131, batches=5) ...\n",
      "#### human_sexuality - accuracy=0.3359 , probability=0.3236\n",
      "### Running MMLU test : international_law (count=121, batches=4) ...\n",
      "#### international_law - accuracy=0.2231 , probability=0.2505\n",
      "### Running MMLU test : jurisprudence (count=108, batches=4) ...\n",
      "#### jurisprudence - accuracy=0.2963 , probability=0.3106\n",
      "### Running MMLU test : logical_fallacies (count=163, batches=6) ...\n",
      "#### logical_fallacies - accuracy=0.2761 , probability=0.2929\n",
      "### Running MMLU test : machine_learning (count=112, batches=4) ...\n",
      "#### machine_learning - accuracy=0.1786 , probability=0.2401\n",
      "### Running MMLU test : management (count=103, batches=4) ...\n",
      "#### management - accuracy=0.5243 , probability=0.4097\n",
      "### Running MMLU test : marketing (count=234, batches=8) ...\n",
      "#### marketing - accuracy=0.3675 , probability=0.3497\n",
      "### Running MMLU test : medical_genetics (count=100, batches=4) ...\n",
      "#### medical_genetics - accuracy=0.2900 , probability=0.2982\n",
      "### Running MMLU test : miscellaneous (count=783, batches=25) ...\n",
      "#### miscellaneous - accuracy=0.3870 , probability=0.3592\n",
      "### Running MMLU test : moral_disputes (count=346, batches=11) ...\n",
      "#### moral_disputes - accuracy=0.2370 , probability=0.2703\n",
      "### Running MMLU test : moral_scenarios (count=895, batches=28) ...\n",
      "#### moral_scenarios - accuracy=0.2737 , probability=0.2607\n",
      "### Running MMLU test : nutrition (count=306, batches=10) ...\n",
      "#### nutrition - accuracy=0.3203 , probability=0.3246\n",
      "### Running MMLU test : philosophy (count=311, batches=10) ...\n",
      "#### philosophy - accuracy=0.3280 , probability=0.3255\n",
      "### Running MMLU test : prehistory (count=324, batches=11) ...\n",
      "#### prehistory - accuracy=0.3025 , probability=0.3171\n",
      "### Running MMLU test : professional_law (count=1534, batches=48) ...\n",
      "#### professional_law - accuracy=0.2451 , probability=0.2576\n",
      "### Running MMLU test : professional_accounting (count=282, batches=9) ...\n",
      "#### professional_accounting - accuracy=0.2482 , probability=0.2571\n",
      "### Running MMLU test : professional_psychology (count=612, batches=20) ...\n",
      "#### professional_psychology - accuracy=0.2614 , probability=0.2793\n",
      "### Running MMLU test : professional_medicine (count=272, batches=9) ...\n",
      "#### professional_medicine - accuracy=0.4522 , probability=0.3619\n",
      "### Running MMLU test : public_relations (count=110, batches=4) ...\n",
      "#### public_relations - accuracy=0.3000 , probability=0.3042\n",
      "### Running MMLU test : security_studies (count=245, batches=8) ...\n",
      "#### security_studies - accuracy=0.4204 , probability=0.3517\n",
      "### Running MMLU test : sociology (count=201, batches=7) ...\n",
      "#### sociology - accuracy=0.3632 , probability=0.3615\n",
      "### Running MMLU test : us_foreign_policy (count=100, batches=4) ...\n",
      "#### us_foreign_policy - accuracy=0.3700 , probability=0.3405\n",
      "### Running MMLU test : virology (count=166, batches=6) ...\n",
      "#### virology - accuracy=0.2590 , probability=0.2899\n",
      "### Running MMLU test : world_religions (count=171, batches=6) ...\n",
      "#### world_religions - accuracy=0.5205 , probability=0.4289\n",
      "------------------------------------------------\n",
      "### MMLU overall test result : accuracy=0.3226 , probability=0.3095\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel (modified)\n",
    "# Batch size of 32, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 32 --n_shot 0 --tmix_backend \"triton\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/v7_goose/.hf_build/v7-1B5-world/\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=world): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV-block/test/mmlu/.mmlu_cache/mmlu-test-t_world-n_0-p_0-c_16-r0.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : abstract_algebra (count=100, batches=4) ...\n",
      "#### abstract_algebra - accuracy=0.2100 , probability=0.2318\n",
      "### Running MMLU test : anatomy (count=135, batches=5) ...\n",
      "#### anatomy - accuracy=0.2963 , probability=0.3041\n",
      "### Running MMLU test : astronomy (count=152, batches=5) ...\n",
      "#### astronomy - accuracy=0.3947 , probability=0.3349\n",
      "### Running MMLU test : business_ethics (count=100, batches=4) ...\n",
      "#### business_ethics - accuracy=0.2800 , probability=0.2811\n",
      "### Running MMLU test : clinical_knowledge (count=265, batches=9) ...\n",
      "#### clinical_knowledge - accuracy=0.3962 , probability=0.3375\n",
      "### Running MMLU test : college_medicine (count=173, batches=6) ...\n",
      "#### college_medicine - accuracy=0.4104 , probability=0.3389\n",
      "### Running MMLU test : college_biology (count=144, batches=5) ...\n",
      "#### college_biology - accuracy=0.2917 , probability=0.3007\n",
      "### Running MMLU test : college_chemistry (count=100, batches=4) ...\n",
      "#### college_chemistry - accuracy=0.4100 , probability=0.3466\n",
      "### Running MMLU test : college_computer_science (count=100, batches=4) ...\n",
      "#### college_computer_science - accuracy=0.3300 , probability=0.3170\n",
      "### Running MMLU test : college_mathematics (count=100, batches=4) ...\n",
      "#### college_mathematics - accuracy=0.3000 , probability=0.2953\n",
      "### Running MMLU test : college_physics (count=102, batches=4) ...\n",
      "#### college_physics - accuracy=0.3824 , probability=0.3195\n",
      "### Running MMLU test : computer_security (count=100, batches=4) ...\n",
      "#### computer_security - accuracy=0.3200 , probability=0.3028\n",
      "### Running MMLU test : conceptual_physics (count=235, batches=8) ...\n",
      "#### conceptual_physics - accuracy=0.2298 , probability=0.2666\n",
      "### Running MMLU test : econometrics (count=114, batches=4) ...\n",
      "#### econometrics - accuracy=0.2456 , probability=0.2438\n",
      "### Running MMLU test : elementary_mathematics (count=378, batches=12) ...\n",
      "#### elementary_mathematics - accuracy=0.2672 , probability=0.2626\n",
      "### Running MMLU test : electrical_engineering (count=145, batches=5) ...\n",
      "#### electrical_engineering - accuracy=0.3034 , probability=0.2793\n",
      "### Running MMLU test : formal_logic (count=126, batches=4) ...\n",
      "#### formal_logic - accuracy=0.3651 , probability=0.2995\n",
      "### Running MMLU test : global_facts (count=100, batches=4) ...\n",
      "#### global_facts - accuracy=0.1800 , probability=0.2339\n",
      "### Running MMLU test : high_school_european_history (count=165, batches=6) ...\n",
      "#### high_school_european_history - accuracy=0.2848 , probability=0.3130\n",
      "### Running MMLU test : high_school_us_history (count=204, batches=7) ...\n",
      "#### high_school_us_history - accuracy=0.2892 , probability=0.3130\n",
      "### Running MMLU test : high_school_world_history (count=237, batches=8) ...\n",
      "#### high_school_world_history - accuracy=0.2236 , probability=0.2737\n",
      "### Running MMLU test : high_school_geography (count=198, batches=7) ...\n",
      "#### high_school_geography - accuracy=0.4394 , probability=0.3834\n",
      "### Running MMLU test : high_school_government_and_politics (count=193, batches=7) ...\n",
      "#### high_school_government_and_politics - accuracy=0.4249 , probability=0.3963\n",
      "### Running MMLU test : high_school_macroeconomics (count=390, batches=13) ...\n",
      "#### high_school_macroeconomics - accuracy=0.3872 , probability=0.3391\n",
      "### Running MMLU test : high_school_microeconomics (count=238, batches=8) ...\n",
      "#### high_school_microeconomics - accuracy=0.3697 , probability=0.3374\n",
      "### Running MMLU test : high_school_psychology (count=545, batches=18) ...\n",
      "#### high_school_psychology - accuracy=0.4642 , probability=0.3949\n",
      "### Running MMLU test : high_school_biology (count=310, batches=10) ...\n",
      "#### high_school_biology - accuracy=0.3548 , probability=0.3389\n",
      "### Running MMLU test : high_school_chemistry (count=203, batches=7) ...\n",
      "#### high_school_chemistry - accuracy=0.2906 , probability=0.2687\n",
      "### Running MMLU test : high_school_computer_science (count=100, batches=4) ...\n",
      "#### high_school_computer_science - accuracy=0.2300 , probability=0.2628\n",
      "### Running MMLU test : high_school_mathematics (count=270, batches=9) ...\n",
      "#### high_school_mathematics - accuracy=0.2630 , probability=0.2558\n",
      "### Running MMLU test : high_school_physics (count=151, batches=5) ...\n",
      "#### high_school_physics - accuracy=0.3311 , probability=0.3010\n",
      "### Running MMLU test : high_school_statistics (count=216, batches=7) ...\n",
      "#### high_school_statistics - accuracy=0.4769 , probability=0.3592\n",
      "### Running MMLU test : human_aging (count=223, batches=7) ...\n",
      "#### human_aging - accuracy=0.1614 , probability=0.2408\n",
      "### Running MMLU test : human_sexuality (count=131, batches=5) ...\n",
      "#### human_sexuality - accuracy=0.3359 , probability=0.3230\n",
      "### Running MMLU test : international_law (count=121, batches=4) ...\n",
      "#### international_law - accuracy=0.2314 , probability=0.2510\n",
      "### Running MMLU test : jurisprudence (count=108, batches=4) ...\n",
      "#### jurisprudence - accuracy=0.3056 , probability=0.3111\n",
      "### Running MMLU test : logical_fallacies (count=163, batches=6) ...\n",
      "#### logical_fallacies - accuracy=0.2761 , probability=0.2929\n",
      "### Running MMLU test : machine_learning (count=112, batches=4) ...\n",
      "#### machine_learning - accuracy=0.1786 , probability=0.2396\n",
      "### Running MMLU test : management (count=103, batches=4) ...\n",
      "#### management - accuracy=0.5146 , probability=0.4104\n",
      "### Running MMLU test : marketing (count=234, batches=8) ...\n",
      "#### marketing - accuracy=0.3761 , probability=0.3494\n",
      "### Running MMLU test : medical_genetics (count=100, batches=4) ...\n",
      "#### medical_genetics - accuracy=0.3000 , probability=0.2990\n",
      "### Running MMLU test : miscellaneous (count=783, batches=25) ...\n",
      "#### miscellaneous - accuracy=0.3819 , probability=0.3592\n",
      "### Running MMLU test : moral_disputes (count=346, batches=11) ...\n",
      "#### moral_disputes - accuracy=0.2370 , probability=0.2705\n",
      "### Running MMLU test : moral_scenarios (count=895, batches=28) ...\n",
      "#### moral_scenarios - accuracy=0.2737 , probability=0.2605\n",
      "### Running MMLU test : nutrition (count=306, batches=10) ...\n",
      "#### nutrition - accuracy=0.3235 , probability=0.3246\n",
      "### Running MMLU test : philosophy (count=311, batches=10) ...\n",
      "#### philosophy - accuracy=0.3280 , probability=0.3253\n",
      "### Running MMLU test : prehistory (count=324, batches=11) ...\n",
      "#### prehistory - accuracy=0.3056 , probability=0.3166\n",
      "### Running MMLU test : professional_law (count=1534, batches=48) ...\n",
      "#### professional_law - accuracy=0.2458 , probability=0.2574\n",
      "### Running MMLU test : professional_accounting (count=282, batches=9) ...\n",
      "#### professional_accounting - accuracy=0.2482 , probability=0.2571\n",
      "### Running MMLU test : professional_psychology (count=612, batches=20) ...\n",
      "#### professional_psychology - accuracy=0.2663 , probability=0.2791\n",
      "### Running MMLU test : professional_medicine (count=272, batches=9) ...\n",
      "#### professional_medicine - accuracy=0.4522 , probability=0.3618\n",
      "### Running MMLU test : public_relations (count=110, batches=4) ...\n",
      "#### public_relations - accuracy=0.3000 , probability=0.3045\n",
      "### Running MMLU test : security_studies (count=245, batches=8) ...\n",
      "#### security_studies - accuracy=0.4204 , probability=0.3514\n",
      "### Running MMLU test : sociology (count=201, batches=7) ...\n",
      "#### sociology - accuracy=0.3532 , probability=0.3606\n",
      "### Running MMLU test : us_foreign_policy (count=100, batches=4) ...\n",
      "#### us_foreign_policy - accuracy=0.3700 , probability=0.3403\n",
      "### Running MMLU test : virology (count=166, batches=6) ...\n",
      "#### virology - accuracy=0.2590 , probability=0.2893\n",
      "### Running MMLU test : world_religions (count=171, batches=6) ...\n",
      "#### world_religions - accuracy=0.5146 , probability=0.4291\n",
      "------------------------------------------------\n",
      "### MMLU overall test result : accuracy=0.3228 , probability=0.3094\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel (modified)\n",
    "# Batch size of 32, is for a 1B5 model, n_shot 0, with 24GB vram (ie. 4090)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 32 --n_shot 0 --tmix_backend \"fla\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py-3-12]",
   "language": "python",
   "name": "conda-env-py-3-12-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
