{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Model filename: v7-1B4.pth\n",
      "### Model n_dim: 2048\n",
      "### model weights keys:\n",
      "emb.weight: torch.Size([50304, 2048]) - torch.bfloat16\n",
      "blocks.0.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.0.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.0.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.0.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.0.ln0.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.0.ln0.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.0.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.0.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.0.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.0.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.0.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.0.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.0.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.0.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.0.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.0.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.0.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.0.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.0.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.0.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.0.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.0.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.0.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.0.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.0.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.0.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.0.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.0.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.0.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.0.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.0.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.0.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.1.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.1.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.1.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.1.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.1.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.1.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.1.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.1.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.1.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.1.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.1.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.1.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.1.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.1.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.1.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.1.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.1.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.1.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.1.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.1.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.1.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.1.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.1.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.1.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.1.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.1.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.1.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.1.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.1.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.1.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.1.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.1.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.1.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.2.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.2.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.2.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.2.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.2.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.2.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.2.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.2.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.2.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.2.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.2.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.2.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.2.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.2.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.2.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.2.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.2.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.2.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.2.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.2.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.2.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.2.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.2.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.2.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.2.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.2.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.2.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.2.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.2.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.2.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.2.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.2.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.2.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.3.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.3.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.3.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.3.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.3.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.3.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.3.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.3.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.3.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.3.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.3.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.3.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.3.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.3.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.3.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.3.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.3.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.3.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.3.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.3.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.3.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.3.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.3.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.3.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.3.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.3.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.3.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.3.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.3.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.3.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.3.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.3.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.3.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.4.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.4.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.4.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.4.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.4.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.4.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.4.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.4.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.4.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.4.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.4.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.4.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.4.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.4.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.4.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.4.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.4.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.4.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.4.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.4.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.4.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.4.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.4.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.4.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.4.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.4.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.4.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.4.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.4.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.4.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.4.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.4.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.4.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.5.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.5.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.5.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.5.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.5.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.5.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.5.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.5.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.5.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.5.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.5.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.5.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.5.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.5.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.5.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.5.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.5.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.5.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.5.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.5.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.5.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.5.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.5.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.5.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.5.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.5.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.5.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.5.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.5.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.5.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.5.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.5.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.5.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.6.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.6.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.6.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.6.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.6.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.6.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.6.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.6.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.6.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.6.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.6.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.6.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.6.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.6.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.6.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.6.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.6.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.6.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.6.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.6.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.6.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.6.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.6.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.6.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.6.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.6.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.6.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.6.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.6.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.6.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.6.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.6.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.6.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.7.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.7.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.7.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.7.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.7.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.7.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.7.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.7.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.7.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.7.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.7.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.7.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.7.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.7.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.7.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.7.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.7.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.7.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.7.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.7.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.7.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.7.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.7.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.7.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.7.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.7.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.7.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.7.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.7.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.7.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.7.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.7.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.7.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.8.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.8.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.8.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.8.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.8.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.8.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.8.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.8.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.8.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.8.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.8.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.8.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.8.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.8.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.8.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.8.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.8.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.8.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.8.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.8.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.8.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.8.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.8.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.8.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.8.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.8.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.8.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.8.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.8.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.8.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.8.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.8.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.8.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.9.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.9.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.9.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.9.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.9.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.9.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.9.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.9.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.9.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.9.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.9.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.9.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.9.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.9.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.9.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.9.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.9.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.9.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.9.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.9.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.9.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.9.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.9.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.9.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.9.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.9.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.9.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.9.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.9.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.9.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.9.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.9.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.9.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.10.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.10.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.10.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.10.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.10.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.10.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.10.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.10.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.10.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.10.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.10.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.10.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.10.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.10.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.10.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.10.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.10.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.10.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.10.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.10.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.10.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.10.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.10.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.10.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.10.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.10.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.10.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.10.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.10.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.10.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.10.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.10.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.10.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.11.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.11.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.11.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.11.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.11.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.11.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.11.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.11.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.11.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.11.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.11.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.11.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.11.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.11.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.11.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.11.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.11.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.11.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.11.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.11.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.11.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.11.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.11.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.11.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.11.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.11.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.11.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.11.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.11.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.11.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.11.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.11.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.11.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.12.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.12.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.12.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.12.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.12.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.12.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.12.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.12.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.12.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.12.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.12.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.12.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.12.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.12.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.12.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.12.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.12.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.12.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.12.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.12.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.12.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.12.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.12.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.12.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.12.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.12.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.12.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.12.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.12.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.12.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.12.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.12.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.12.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.13.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.13.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.13.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.13.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.13.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.13.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.13.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.13.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.13.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.13.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.13.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.13.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.13.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.13.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.13.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.13.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.13.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.13.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.13.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.13.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.13.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.13.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.13.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.13.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.13.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.13.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.13.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.13.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.13.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.13.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.13.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.13.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.13.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.14.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.14.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.14.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.14.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.14.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.14.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.14.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.14.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.14.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.14.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.14.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.14.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.14.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.14.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.14.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.14.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.14.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.14.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.14.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.14.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.14.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.14.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.14.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.14.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.14.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.14.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.14.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.14.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.14.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.14.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.14.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.14.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.14.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.15.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.15.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.15.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.15.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.15.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.15.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.15.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.15.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.15.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.15.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.15.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.15.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.15.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.15.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.15.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.15.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.15.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.15.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.15.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.15.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.15.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.15.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.15.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.15.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.15.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.15.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.15.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.15.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.15.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.15.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.15.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.15.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.15.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.16.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.16.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.16.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.16.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.16.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.16.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.16.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.16.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.16.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.16.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.16.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.16.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.16.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.16.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.16.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.16.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.16.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.16.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.16.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.16.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.16.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.16.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.16.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.16.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.16.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.16.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.16.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.16.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.16.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.16.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.16.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.16.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.16.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.17.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.17.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.17.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.17.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.17.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.17.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.17.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.17.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.17.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.17.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.17.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.17.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.17.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.17.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.17.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.17.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.17.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.17.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.17.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.17.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.17.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.17.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.17.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.17.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.17.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.17.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.17.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.17.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.17.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.17.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.17.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.17.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.17.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.18.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.18.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.18.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.18.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.18.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.18.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.18.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.18.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.18.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.18.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.18.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.18.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.18.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.18.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.18.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.18.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.18.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.18.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.18.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.18.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.18.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.18.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.18.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.18.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.18.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.18.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.18.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.18.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.18.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.18.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.18.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.18.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.18.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.19.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.19.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.19.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.19.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.19.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.19.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.19.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.19.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.19.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.19.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.19.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.19.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.19.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.19.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.19.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.19.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.19.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.19.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.19.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.19.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.19.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.19.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.19.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.19.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.19.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.19.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.19.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.19.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.19.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.19.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.19.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.19.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.19.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.20.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.20.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.20.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.20.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.20.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.20.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.20.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.20.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.20.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.20.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.20.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.20.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.20.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.20.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.20.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.20.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.20.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.20.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.20.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.20.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.20.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.20.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.20.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.20.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.20.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.20.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.20.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.20.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.20.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.20.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.20.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.20.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.20.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.21.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.21.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.21.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.21.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.21.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.21.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.21.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.21.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.21.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.21.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.21.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.21.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.21.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.21.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.21.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.21.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.21.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.21.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.21.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.21.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.21.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.21.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.21.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.21.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.21.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.21.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.21.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.21.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.21.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.21.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.21.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.21.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.21.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.22.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.22.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.22.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.22.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.22.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.22.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.22.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.22.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.22.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.22.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.22.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.22.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.22.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.22.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.22.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.22.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.22.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.22.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.22.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.22.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.22.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.22.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.22.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.22.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.22.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.22.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.22.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.22.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.22.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.22.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.22.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.22.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.22.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "blocks.23.ln1.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.23.ln1.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.23.ln2.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.23.ln2.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.23.att.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.23.att.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.23.att.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.23.att.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.23.att.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.23.att.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.23.att.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.23.att.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "blocks.23.att.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.23.att.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.23.att.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "blocks.23.att.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "blocks.23.att.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.23.att.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "blocks.23.att.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "blocks.23.att.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "blocks.23.att.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "blocks.23.att.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.23.att.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.23.att.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.23.att.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.23.att.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.23.att.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.23.att.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "blocks.23.att.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.23.att.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "blocks.23.ffn.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "blocks.23.ffn.key.weight: torch.Size([8192, 2048]) - torch.bfloat16\n",
      "blocks.23.ffn.value.weight: torch.Size([2048, 8192]) - torch.bfloat16\n",
      "ln_out.weight: torch.Size([2048]) - torch.bfloat16\n",
      "ln_out.bias: torch.Size([2048]) - torch.bfloat16\n",
      "head.weight: torch.Size([50304, 2048]) - torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Configure the parent path to be the proj folder\n",
    "import sys, os, torch, time\n",
    "sys.path.append('../../')\n",
    "\n",
    "# Import the block classes\n",
    "from rwkv.v7_goose.block.rwkv7_time_mix import RWKV7TimeMix\n",
    "\n",
    "# File to load\n",
    "MODEL_FILENAME=\"v7-1B4.pth\"\n",
    "\n",
    "# Run device, and run dtype to use\n",
    "RUN_DEVICE=\"cpu\"\n",
    "RUN_DTYPE=torch.bfloat16\n",
    "\n",
    "# Check for cuda device\n",
    "if torch.cuda.is_available():\n",
    "    RUN_DEVICE=\"cuda:0\"\n",
    "\n",
    "# Check if the reference weights exists\n",
    "assert os.path.exists(f\"./.model/{MODEL_FILENAME}\"), \"The reference weights does not exist. Please download it first (00-model-download.ipynb)\"\n",
    "\n",
    "# Loads the model weights\n",
    "model_weight = torch.load(f\"./.model/{MODEL_FILENAME}\", map_location='cpu', weights_only=True, mmap=True)\n",
    "\n",
    "# Model filename\n",
    "print(f\"### Model filename: {MODEL_FILENAME}\")\n",
    "\n",
    "# Lets get the n_dim, and setup the test module\n",
    "n_dim = model_weight['emb.weight'].shape[1]\n",
    "print(f\"### Model n_dim: {n_dim}\")\n",
    "\n",
    "# List the model weights keys, and their shapes\n",
    "print(f\"### model weights keys:\")\n",
    "for key in model_weight:\n",
    "    print(f\"{key}: {model_weight[key].shape} - {model_weight[key].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tmix named parameters:\n",
      "x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "gate.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "### tmix state keys:\n",
      "tmix.x_r: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "tmix.x_w: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "tmix.x_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "tmix.x_v: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "tmix.x_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "tmix.x_g: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "tmix.w1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "tmix.w2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "tmix.w0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "tmix.a1: torch.Size([2048, 96]) - torch.bfloat16\n",
      "tmix.a2: torch.Size([96, 2048]) - torch.bfloat16\n",
      "tmix.a0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "tmix.v1: torch.Size([2048, 64]) - torch.bfloat16\n",
      "tmix.v2: torch.Size([64, 2048]) - torch.bfloat16\n",
      "tmix.v0: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "tmix.g1: torch.Size([2048, 256]) - torch.bfloat16\n",
      "tmix.g2: torch.Size([256, 2048]) - torch.bfloat16\n",
      "tmix.k_k: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "tmix.k_a: torch.Size([1, 1, 2048]) - torch.bfloat16\n",
      "tmix.r_k: torch.Size([32, 64]) - torch.bfloat16\n",
      "tmix.receptance.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "tmix.key.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "tmix.value.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "tmix.gate.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "tmix.output.weight: torch.Size([2048, 2048]) - torch.bfloat16\n",
      "tmix.ln_x.weight: torch.Size([2048]) - torch.bfloat16\n",
      "tmix.ln_x.bias: torch.Size([2048]) - torch.bfloat16\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Initialize the channelmix state, and x state to test\n",
    "IN_TOKENS_LEN=8192\n",
    "x_state_0 = torch.ones(1, IN_TOKENS_LEN, n_dim, device=RUN_DEVICE, dtype=RUN_DTYPE)\n",
    "x_state_1 = torch.ones(1, IN_TOKENS_LEN, n_dim, device=RUN_DEVICE, dtype=RUN_DTYPE)\n",
    "x_state_2 = torch.ones(1, IN_TOKENS_LEN, n_dim, device=RUN_DEVICE, dtype=RUN_DTYPE)\n",
    "tmix_shift_0 = torch.ones(1, n_dim, device=RUN_DEVICE, dtype=RUN_DTYPE)\n",
    "tmix_shift_1 = torch.ones(1, n_dim, device=RUN_DEVICE, dtype=RUN_DTYPE)\n",
    "tmix_wkv_0 = torch.ones(1, n_dim // 64, 64, 64, device=RUN_DEVICE, dtype=RUN_DTYPE)\n",
    "tmix_wkv_1 = torch.ones(1, n_dim // 64, 64, 64, device=RUN_DEVICE, dtype=RUN_DTYPE)\n",
    "\n",
    "# Build the cmix block\n",
    "tmix = RWKV7TimeMix({ \"n_layer\":24, \"n_dim\":n_dim, \"layer_id\":0, \"device\":RUN_DEVICE, \"dtype\":RUN_DTYPE })\n",
    "tmix.load_from_model_state_dict(model_weight, 0)\n",
    "\n",
    "# Get the named parameters\n",
    "tmix_params = tmix.named_parameters()\n",
    "print(f\"### tmix named parameters:\")\n",
    "for name, param in tmix_params:\n",
    "    print(f\"{name}: {param.shape} - {param.dtype}\")\n",
    "\n",
    "# Log each item shape\n",
    "tmix_state = tmix.state_dict()\n",
    "print(f\"### tmix state keys:\")\n",
    "for key in tmix_state:\n",
    "    print(f\"tmix.{key}: {tmix_state[key].shape} - {tmix_state[key].dtype}\")\n",
    "print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tmix forward passes (warmup): 16927.453184127808 ms (cpu, torch.bfloat16)\n",
      "1 tmix forward passes (normal): 17133.623361587524 ms (cpu, torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# Iteration to test\n",
    "TEST_STEPS = 5\n",
    "\n",
    "### TMix\n",
    "with torch.inference_mode():\n",
    "\n",
    "    # This is a warmup\n",
    "    t0 = time.time()\n",
    "    out_x = x_state_0\n",
    "    t_shift = tmix_shift_0\n",
    "    t_wkv = tmix_wkv_0\n",
    "    v_first = x_state_2\n",
    "    for i in range(TEST_STEPS):\n",
    "        out_x, t_shift, t_wkv, v_first = tmix(x_state_1, t_shift, tmix_wkv_1, v_first)\n",
    "    t2 = time.time()\n",
    "    print(f'1 tmix forward passes (warmup): {(t2-t0)*1000/TEST_STEPS} ms ({RUN_DEVICE}, {RUN_DTYPE})')\n",
    "\n",
    "    # The actual run\n",
    "    t1 = time.time()\n",
    "    out_x = x_state_0\n",
    "    t_shift = tmix_shift_0\n",
    "    t_wkv = tmix_wkv_0\n",
    "    v_first = x_state_2\n",
    "    for i in range(TEST_STEPS):\n",
    "        out_x, t_shift, t_wkv, v_first = tmix(x_state_1, t_shift, tmix_wkv_1, v_first)\n",
    "    t2 = time.time()\n",
    "    print(f'1 tmix forward passes (normal): {(t2-t1)*1000/TEST_STEPS} ms ({RUN_DEVICE}, {RUN_DTYPE})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m v_first \u001b[38;5;241m=\u001b[39m x_state_2\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(TEST_STEPS):\n\u001b[0;32m---> 14\u001b[0m     out_x, t_shift, t_wkv, v_first \u001b[38;5;241m=\u001b[39m \u001b[43mtmix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_with_default_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_state_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_shift\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmix_wkv_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_shift\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_wkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 tmix forward passes (warmup): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(t2\u001b[38;5;241m-\u001b[39mt0)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;241m/\u001b[39mTEST_STEPS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRUN_DEVICE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRUN_DTYPE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:433\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    429\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    437\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:1116\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[1;32m   1111\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[1;32m   1112\u001b[0m             )\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:948\u001b[0m, in \u001b[0;36mConvertFrame.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    946\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 948\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:472\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    458\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[1;32m    460\u001b[0m signpost_event(\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m     },\n\u001b[1;32m    470\u001b[0m )\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_utils_internal.py:84\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     83\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStrobelightCompileTimeProfiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofile_compile_time\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_strobelight/compile_time_profiler.py:129\u001b[0m, in \u001b[0;36mStrobelightCompileTimeProfiler.profile_compile_time\u001b[0;34m(cls, func, phase_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprofile_compile_time\u001b[39m(\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mcls\u001b[39m, func: Any, phase_name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m--> 129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprofiler is not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:817\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    815\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 817\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    820\u001b[0m     Unsupported,\n\u001b[1;32m    821\u001b[0m     TorchRuntimeError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    828\u001b[0m     BisectValidationException,\n\u001b[1;32m    829\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:636\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    634\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 636\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py:1185\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1182\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1183\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1185\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:178\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m cleanup \u001b[38;5;241m=\u001b[39m setup_compile_debug()\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:582\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 582\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    584\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:2451\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2451\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:893\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:805\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException:\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:2642\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mRETURN_VALUE\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[0;32m-> 2642\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:2627\u001b[0m, in \u001b[0;36mInstructionTranslator._return\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2622\u001b[0m _step_logger()(\n\u001b[1;32m   2623\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[1;32m   2624\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2625\u001b[0m )\n\u001b[1;32m   2626\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m, inst\u001b[38;5;241m.\u001b[39mopname)\n\u001b[0;32m-> 2627\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   2631\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2632\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2633\u001b[0m return_inst \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2634\u001b[0m     create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2636\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_CONST\u001b[39m\u001b[38;5;124m\"\u001b[39m, argval\u001b[38;5;241m=\u001b[39minst\u001b[38;5;241m.\u001b[39margval)\n\u001b[1;32m   2637\u001b[0m )\n\u001b[1;32m   2638\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([return_inst])\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1123\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m   1120\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count_calls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pass2\u001b[38;5;241m.\u001b[39mgraph_outputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1122\u001b[0m     output\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m-> 1123\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_output_vars\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1124\u001b[0m     )\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pass2\u001b[38;5;241m.\u001b[39mgraph_outputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1127\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(pass2\u001b[38;5;241m.\u001b[39mcreate_store(graph_output_var))\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1318\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[0;32m-> 1318\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lazy_graph_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LazyGraphModule\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_fn, _LazyGraphModule) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(compiled_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), _LazyGraphModule)\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m compiled_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lazy_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[38;5;66;03m# this is a _LazyGraphModule. This makes it easier for dynamo to\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;66;03m# optimize a _LazyGraphModule.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1390\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverify_correctness:\n\u001b[1;32m   1389\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[0;32m-> 1390\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_fn did not return callable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py:129\u001b[0m, in \u001b[0;36mWrapBackendDebug.__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/__init__.py:1951\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[1;32m   1949\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 1951\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:1505\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m inference_compiler(unlifted_gm, example_inputs_)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode), torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mtracing(\n\u001b[1;32m   1503\u001b[0m     tracing_context\n\u001b[1;32m   1504\u001b[0m ), compiled_autograd\u001b[38;5;241m.\u001b[39mdisable(), functorch_config\u001b[38;5;241m.\u001b[39mpatch(unlift_effect_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m-> 1505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/backends/common.py:69\u001b[0m, in \u001b[0;36mAotAutograd.__call__\u001b[0;34m(self, gm, example_inputs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# NB: NOT cloned!\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[0;32m---> 69\u001b[0m         cg \u001b[38;5;241m=\u001b[39m \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m         counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m disable(cg)\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py:954\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler)\u001b[0m\n\u001b[1;32m    938\u001b[0m aot_config \u001b[38;5;241m=\u001b[39m AOTConfig(\n\u001b[1;32m    939\u001b[0m     fw_compiler\u001b[38;5;241m=\u001b[39mfw_compiler,\n\u001b[1;32m    940\u001b[0m     bw_compiler\u001b[38;5;241m=\u001b[39mbw_compiler,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    950\u001b[0m     no_tangents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    951\u001b[0m )\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[0;32m--> 954\u001b[0m     compiled_fn, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mGmWrapper):\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;66;03m# This function is called by the flatten_graph_inputs wrapper, which boxes\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;66;03m# the inputs so that they can be freed before the end of this scope.\u001b[39;00m\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;66;03m# For overhead reasons, this is not the default wrapper, see comment:\u001b[39;00m\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/122535/files#r1560096481\u001b[39;00m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mboxed_forward\u001b[39m(runtime_args: List[Any]):\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py:687\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m aot_dispatch_base\n\u001b[1;32m    685\u001b[0m compiler_fn \u001b[38;5;241m=\u001b[39m choose_dispatcher(needs_autograd, aot_config)\n\u001b[0;32m--> 687\u001b[0m compiled_fn, fw_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn, fw_metadata\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:168\u001b[0m, in \u001b[0;36maot_dispatch_base\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[1;32m    161\u001b[0m     tracing_context\u001b[38;5;241m.\u001b[39mfw_metadata \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m         fw_metadata\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m maybe_subclass_meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m maybe_subclass_meta\u001b[38;5;241m.\u001b[39mfw_metadata\n\u001b[1;32m    165\u001b[0m     )\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TracingContext\u001b[38;5;241m.\u001b[39mreport_output_strides() \u001b[38;5;28;01mas\u001b[39;00m fwd_output_strides:\n\u001b[0;32m--> 168\u001b[0m     compiled_fw \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfw_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_flat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fakified_out_wrapper\u001b[38;5;241m.\u001b[39mneeds_post_compile:\n\u001b[1;32m    171\u001b[0m     fakified_out_wrapper\u001b[38;5;241m.\u001b[39mset_fwd_output_strides(fwd_output_strides)\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:1410\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.fw_compiler_base\u001b[0;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m orig_output_end_idx \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m num_model_outputs\n\u001b[1;32m   1404\u001b[0m     user_visible_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys(\n\u001b[1;32m   1405\u001b[0m         n\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m   1406\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m model_outputs[original_output_start_index:orig_output_end_idx]\n\u001b[1;32m   1407\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mNode)\n\u001b[1;32m   1408\u001b[0m     )\n\u001b[0;32m-> 1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_input_idxs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_static_input_idxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfixed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxed_forward_device_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforward_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_visible_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_visible_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1419\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/repro/after_aot.py:84\u001b[0m, in \u001b[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# Call the compiler_fn - which is either aot_autograd or inductor\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# with fake inputs\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     inner_compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# TODO: Failures here are troublesome because no real inputs,\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# need a different serialization strategy\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_inductor/debug.py:304\u001b[0m, in \u001b[0;36mDebugContext.wrap.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m DebugContext():\n\u001b[0;32m--> 304\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_dynamo/utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:527\u001b[0m, in \u001b[0;36mcompile_fx_inner\u001b[0;34m(gm, example_inputs, cudagraphs, static_input_idxs, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, boxed_forward_device_index, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[1;32m    517\u001b[0m     compiled_graph \u001b[38;5;241m=\u001b[39m FxGraphCache\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    518\u001b[0m         fx_codegen_and_compile,\n\u001b[1;32m    519\u001b[0m         gm,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         remote\u001b[38;5;241m=\u001b[39mfx_graph_remote_cache,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFX codegen and compilation took \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# check cudagraph disabling reasons from inductor lowering\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:738\u001b[0m, in \u001b[0;36mfx_codegen_and_compile\u001b[0;34m(gm, example_inputs, cudagraphs, static_input_idxs, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;66;03m# It is safe to run FakeTensorProp under no_grad because by the time\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;66;03m# we're in inductor, we assume that AOTAutograd has already \"taken care\"\u001b[39;00m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;66;03m# of autograd, so there should be no more autograd-related API's in the\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;66;03m# graph.\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 738\u001b[0m     fake_mode \u001b[38;5;241m=\u001b[39m \u001b[43mfake_tensor_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;66;03m# pattern matcher passes might not preserve striding information\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;66;03m# on node.meta[\"val\"]. if in the future we rely on these being\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# correct we will need to fix.\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode):\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;66;03m# has some issues with memory in training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:379\u001b[0m, in \u001b[0;36mfake_tensor_prop\u001b[0;34m(gm, example_inputs, force_allow_non_fake_inputs)\u001b[0m\n\u001b[1;32m    373\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    374\u001b[0m         contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force_allow_non_fake_inputs\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m mock\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mobject(fake_mode, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_non_fake_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    377\u001b[0m     )\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m         \u001b[43mFakeTensorProp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfake_mode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate_dont_convert_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fake_mode\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/fx/passes/fake_tensor_prop.py:69\u001b[0m, in \u001b[0;36mFakeTensorProp.propagate_dont_convert_inputs\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpropagate_dont_convert_inputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode:\n\u001b[0;32m---> 69\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/fx/interpreter.py:146\u001b[0m, in \u001b[0;36mInterpreter.run\u001b[0;34m(self, initial_env, enable_io_processing, *args)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv[node] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_traceback:\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/fx/passes/fake_tensor_prop.py:37\u001b[0m, in \u001b[0;36mFakeTensorProp.run_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_node\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: Node):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rebind_unbacked, compute_unbacked_bindings\n\u001b[0;32m---> 37\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     rebind_unbacked(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\u001b[38;5;241m.\u001b[39mshape_env, n, result)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_val\u001b[39m(obj):\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/fx/interpreter.py:203\u001b[0m, in \u001b[0;36mInterpreter.run_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kwargs, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/fx/interpreter.py:275\u001b[0m, in \u001b[0;36mInterpreter.call_function\u001b[0;34m(self, target, args, kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Execute the function and return the result\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_ops.py:667\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(self_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;66;03m# are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/utils/_stats.py:21\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:1061\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   1058\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_dispatch_mode(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_TorchDispatchModeKey\u001b[38;5;241m.\u001b[39mFAKE) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m ), func\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake tensor raised TypeError\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:1450\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_enabled:\n\u001b[0;32m-> 1450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_impl(func, types, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:1153\u001b[0m, in \u001b[0;36mFakeTensorMode._cached_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     FakeTensorMode\u001b[38;5;241m.\u001b[39mcache_bypasses[e\u001b[38;5;241m.\u001b[39mreason] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m _UNASSIGNED:\n\u001b[0;32m-> 1153\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:1694\u001b[0m, in \u001b[0;36mFakeTensorMode._dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m decomposition_table[func](\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;66;03m# Decomposes CompositeImplicitAutograd ops\u001b[39;00m\n\u001b[0;32m-> 1694\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1696\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_ops.py:704\u001b[0m, in \u001b[0;36mOpOverload.decompose\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpy_kernels[dk](\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_dispatch_has_kernel_for_dispatch_key(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname(), dk):\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op_dk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/utils/_stats.py:21\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:1061\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   1058\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_dispatch_mode(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_TorchDispatchModeKey\u001b[38;5;241m.\u001b[39mFAKE) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m ), func\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake tensor raised TypeError\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:1450\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_enabled:\n\u001b[0;32m-> 1450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_impl(func, types, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:1137\u001b[0m, in \u001b[0;36mFakeTensorMode._cached_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1135\u001b[0m entry \u001b[38;5;241m=\u001b[39m FakeTensorMode\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m entry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1137\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_from_cache_entry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m     FakeTensorMode\u001b[38;5;241m.\u001b[39mcache_hits \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_crosscheck_enabled:\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;66;03m# For debugging / testing: Validate that the output synthesized\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;66;03m# from the cache matches the output created by normal dispatch.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:1385\u001b[0m, in \u001b[0;36mFakeTensorMode._output_from_cache_entry\u001b[0;34m(self, entry, func, args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39mis_view:\n\u001b[1;32m   1383\u001b[0m     \u001b[38;5;66;03m# For view ops, the storage should be the same as the tensor input.\u001b[39;00m\n\u001b[1;32m   1384\u001b[0m     storage \u001b[38;5;241m=\u001b[39m args[cast(\u001b[38;5;28mint\u001b[39m, entry\u001b[38;5;241m.\u001b[39mview_idx)]\u001b[38;5;241m.\u001b[39muntyped_storage()\n\u001b[0;32m-> 1385\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m in_kernel_invocation_manager(\u001b[38;5;28mself\u001b[39m), maybe_suppress():\n\u001b[1;32m   1386\u001b[0m         empty\u001b[38;5;241m.\u001b[39mset_(\n\u001b[1;32m   1387\u001b[0m             storage, metadata\u001b[38;5;241m.\u001b[39mstorage_offset, metadata\u001b[38;5;241m.\u001b[39mshape, metadata\u001b[38;5;241m.\u001b[39mstride\n\u001b[1;32m   1388\u001b[0m         )\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metadata\u001b[38;5;241m.\u001b[39mstorage_offset \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:2984\u001b[0m, in \u001b[0;36mShapeEnv.suppress_guards\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[1;32m   2982\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msuppress_guards\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2983\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager to ignore all guards generated inside\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2984\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_suppress_guards_enter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2986\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/fx/experimental/recording.py:245\u001b[0m, in \u001b[0;36mrecord_shapeenv_event.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_recording:  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;66;03m# If ShapeEnv is already recording an event, call the wrapped\u001b[39;00m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;66;03m# function directly.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# NB: here, we skip the check of whether all ShapeEnv instances\u001b[39;00m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;66;03m# are equal, in favor of a faster dispatch.\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m# Retrieve an instance of ShapeEnv.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# Assumption: the collection of args and kwargs may not reference\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# different ShapeEnv instances.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m _extract_shape_env_and_assert_equal(args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/python-3-12/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:2973\u001b[0m, in \u001b[0;36mShapeEnv._suppress_guards_enter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2970\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_suppress_guards_tls\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(TLS, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuppress_guards\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 2973\u001b[0m \u001b[38;5;129m@record_shapeenv_event\u001b[39m()\n\u001b[1;32m   2974\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_suppress_guards_enter\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2975\u001b[0m     TLS\u001b[38;5;241m.\u001b[39msuppress_guards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2977\u001b[0m \u001b[38;5;129m@record_shapeenv_event\u001b[39m()\n\u001b[1;32m   2978\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_suppress_guards_exit\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iteration to test\n",
    "TEST_STEPS = 5\n",
    "\n",
    "### TMix\n",
    "with torch.inference_mode():\n",
    "\n",
    "    # This is a warmup\n",
    "    t0 = time.time()\n",
    "    out_x = x_state_0\n",
    "    t_shift = tmix_shift_0\n",
    "    t_wkv = tmix_wkv_0\n",
    "    v_first = x_state_2\n",
    "    for i in range(TEST_STEPS):\n",
    "        out_x, t_shift, t_wkv, v_first = tmix.forward_with_default_compile(x_state_1, t_shift, tmix_wkv_1, v_first, out_x, t_shift, t_wkv, v_first)\n",
    "    t2 = time.time()\n",
    "    print(f'1 tmix forward passes (warmup): {(t2-t0)*1000/TEST_STEPS} ms ({RUN_DEVICE}, {RUN_DTYPE})')\n",
    "\n",
    "    # The actual run\n",
    "    t1 = time.time()\n",
    "    out_x = x_state_0\n",
    "    t_shift = tmix_shift_0\n",
    "    t_wkv = tmix_wkv_0\n",
    "    v_first = x_state_2\n",
    "    for i in range(TEST_STEPS):\n",
    "        out_x, t_shift, t_wkv, v_first = tmix.forward_with_default_compile(x_state_1, t_shift, tmix_wkv_1, v_first, out_x, t_shift, t_wkv, v_first)\n",
    "    t2 = time.time()\n",
    "    print(f'1 tmix forward passes (compiled): {(t2-t1)*1000/TEST_STEPS} ms ({RUN_DEVICE}, {RUN_DTYPE})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration to test\n",
    "TEST_STEPS = 5\n",
    "\n",
    "### TMix\n",
    "with torch.inference_mode():\n",
    "\n",
    "    # This is a warmup\n",
    "    t0 = time.time()\n",
    "    out_x = x_state_0\n",
    "    t_shift = tmix_shift_0\n",
    "    t_wkv = tmix_wkv_0\n",
    "    v_first = x_state_2\n",
    "    for i in range(TEST_STEPS):\n",
    "        out_x, t_shift, t_wkv, v_first = tmix.forward_with_reduce_compile(x_state_1, t_shift, tmix_wkv_1, v_first)\n",
    "    t2 = time.time()\n",
    "    print(f'1 tmix forward passes (warmup): {(t2-t0)*1000/TEST_STEPS} ms ({RUN_DEVICE}, {RUN_DTYPE})')\n",
    "\n",
    "    # The actual run\n",
    "    t1 = time.time()\n",
    "    out_x = x_state_0\n",
    "    t_shift = tmix_shift_0\n",
    "    t_wkv = tmix_wkv_0\n",
    "    v_first = x_state_2\n",
    "    for i in range(TEST_STEPS):\n",
    "        out_x, t_shift, t_wkv, v_first = tmix.forward_with_reduce_compile(x_state_1, t_shift, tmix_wkv_1, v_first)\n",
    "    t2 = time.time()\n",
    "    print(f'1 tmix forward passes (normal): {(t2-t1)*1000/TEST_STEPS} ms ({RUN_DEVICE}, {RUN_DTYPE})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export tmix1 state dict\n",
    "tmix_state = tmix.state_dict()\n",
    "\n",
    "# Log each item shape\n",
    "print(f\"### tmix state keys:\")\n",
    "for key in tmix_state:\n",
    "    print(f\"tmix.{key}: {tmix_state[key].shape} - {tmix_state[key].dtype}\")\n",
    "print(\"----\")\n",
    "\n",
    "# Build the tmix block\n",
    "tmix2 = RWKV7TimeMix({ \"n_layer\":24, \"n_dim\":n_dim, \"layer_id\":1, \"tmix_backend\":\"torch\", \"device\":RUN_DEVICE, \"dtype\":RUN_DTYPE })\n",
    "\n",
    "# Load the state dict\n",
    "tmix2.load_state_dict(tmix_state)\n",
    "\n",
    "# Log each item shape\n",
    "print(f\"### tmix2 state keys:\")\n",
    "for key in tmix_state:\n",
    "    print(f\"tmix.{key}: {tmix_state[key].shape} - {tmix_state[key].dtype}\")\n",
    "print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
