{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training validation\n",
    "\n",
    "CUDA based training, for loss curve comparision / validation between the various kernel implements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "[SimpleTestTrainer] Initializing the trainer for:  RWKV-Block.SimpleTestTrainer\n",
      "- hf_dataset:          recursal/SuperWiki-Tiny\n",
      "- dataset_ctx_length:  4096\n",
      "- dataset_min_length:  4096\n",
      "- tokenizer_name:      EleutherAI/gpt-neox-20b\n",
      "- batch_size:          4\n",
      "- learning_rate:       0.001\n",
      "- num_epochs:          1\n",
      "---------------------------------------------\n",
      "[SimpleTestTrainer] Loading the tokenizer:  EleutherAI/gpt-neox-20b ...\n",
      "[SimpleTestTrainer] Loading the dataset:  recursal/SuperWiki-Tiny ...\n",
      "[SimpleTestTrainer] Preparing the training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SimpleTestTrainer] Training dataset size:    718044\n",
      "[SimpleTestTrainer] Validation dataset size:  719\n",
      "[SimpleTestTrainer] Preparing the data loaders...\n",
      "[SimpleTestTrainer] Training batch count:    179511\n",
      "[SimpleTestTrainer] Validation batch count:  179\n",
      "[SimpleTestTrainer] Setting up the optimizer, loss function...\n",
      "[SimpleTestTrainer] Initializing wandb...\n",
      "[SimpleTestTrainer] wandb is logged in.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/recursal/rwkv-prj/RWKV-block/test/v7_goose/wandb/run-20250116_214726-e9g5jiod</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rwkv-x-dev/RWKV-Block.SimpleTestTrainer/runs/e9g5jiod' target=\"_blank\">super-cloud-82</a></strong> to <a href='https://wandb.ai/rwkv-x-dev/RWKV-Block.SimpleTestTrainer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rwkv-x-dev/RWKV-Block.SimpleTestTrainer' target=\"_blank\">https://wandb.ai/rwkv-x-dev/RWKV-Block.SimpleTestTrainer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rwkv-x-dev/RWKV-Block.SimpleTestTrainer/runs/e9g5jiod' target=\"_blank\">https://wandb.ai/rwkv-x-dev/RWKV-Block.SimpleTestTrainer/runs/e9g5jiod</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SimpleTestTrainer] Initialization complete.\n",
      "---------------------------------------------\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                                                                                          | 0/179511 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Reference CUDA kernel does not support input RWKV state, and is used only for training/validaiton purposes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/recursal/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py312_cu121/wind_backstepping/build.ninja...\n",
      "/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module wind_backstepping...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module wind_backstepping...\n",
      "Training:   0%|                                                                                                                                    | 1/179511 [00:01<75:10:42,  1.51s/it, loss=11.1]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.16 GiB. GPU 3 has a total capacity of 23.65 GiB of which 5.09 GiB is free. Including non-PyTorch memory, this process has 18.55 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SimpleTestTrainer(model, device\u001b[38;5;241m=\u001b[39mRUN_DEVICE, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Trigger the train process\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rwkv-prj/RWKV-block/test/v7_goose/../../test/trainer/SimpleTestTrainer.py:249\u001b[0m, in \u001b[0;36mSimpleTestTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 249\u001b[0m     epoch_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     epoch_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/rwkv-prj/RWKV-block/test/v7_goose/../../test/trainer/SimpleTestTrainer.py:202\u001b[0m, in \u001b[0;36mSimpleTestTrainer.train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(input_ids)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m,      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN detected in the input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(label)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m,          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN detected in the label\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_t_logits\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN detected in the model output\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m index \u001b[38;5;241m=\u001b[39m batch_t_logits[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_t_logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    205\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(index, label)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.16 GiB. GPU 3 has a total capacity of 23.65 GiB of which 5.09 GiB is free. Including non-PyTorch memory, this process has 18.55 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Configure the parent path to be the proj folder\n",
    "import sys, os, torch, time\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../test')\n",
    "\n",
    "# Memory segmenting fix?\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Ensure sys.path has export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}\n",
    "# so that the CUDA binaries are available\n",
    "if \"/cuda/bin\" not in sys.path:\n",
    "    sys.path.append('/usr/local/cuda/bin')\n",
    "\n",
    "# Import the model classes\n",
    "from rwkv_block.v7_goose.model.rwkv7_goose_model import RWKV7GooseModel\n",
    "from trainer.SimpleTestTrainer import SimpleTestTrainer\n",
    "\n",
    "# Device to run on\n",
    "RUN_DEVICE=\"cuda:0\"\n",
    "\n",
    "# If multiple cuda devices are available\n",
    "# we use the respective device, so that I can run multiple notebooks in parallel\n",
    "#\n",
    "# Comment out this logic if you intend to manually set the device\n",
    "if torch.cuda.device_count() >= 8:\n",
    "    RUN_DEVICE=\"cuda:3\"\n",
    "\n",
    "# Training batch size\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Model shape and size\n",
    "LAYER_COUNT = 12\n",
    "DIM_SIZE = 512\n",
    "TMIX_BACKEND=\"cuda_ref\"\n",
    "\n",
    "# Create and initalize the model\n",
    "model = RWKV7GooseModel({\n",
    "    \"n_layer\": LAYER_COUNT,\n",
    "    \"n_dim\": DIM_SIZE,\n",
    "    \"tmix_backend\": TMIX_BACKEND,\n",
    "    \"device\": RUN_DEVICE,\n",
    "    \"dtype\": \"bfloat16\",\n",
    "    \"n_vocab\": 50432\n",
    "})\n",
    "model.init_parameters()\n",
    "\n",
    "# Setup the trainer\n",
    "trainer = SimpleTestTrainer(model, device=RUN_DEVICE, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Trigger the train process\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py-3-12]",
   "language": "python",
   "name": "conda-env-py-3-12-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
