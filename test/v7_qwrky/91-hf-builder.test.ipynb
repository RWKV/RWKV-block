{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face builder test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_qwrky\n",
      "Model file path: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_qwrky/.model/qwrky7-7B.pth\n",
      "Project directory: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block\n",
      "Output directory: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_qwrky/.hf_build/qwrky7-7B/\n"
     ]
    }
   ],
   "source": [
    "# Get the current script directory, from the notebook\n",
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "print(\"Notebook directory:\", notebook_dir)\n",
    "\n",
    "model_filename = \"qwrky7-7B\"\n",
    "model_file = os.path.join(notebook_dir, \".model\", f\"{model_filename}.pth\")\n",
    "print(\"Model file path:\", model_file)\n",
    "\n",
    "# Check if the model file exists\n",
    "if os.path.isfile(model_file) is False:\n",
    "    raise Exception(\"Model file does not exist\")\n",
    "\n",
    "# Get the project directory two levels up\n",
    "project_dir = os.path.dirname(os.path.dirname(notebook_dir))\n",
    "print(\"Project directory:\", project_dir)\n",
    "\n",
    "# Output build directory\n",
    "output_dir = os.path.join(notebook_dir, f\".hf_build/{model_filename}/\")\n",
    "print(\"Output directory:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Converting RWKV model to HuggingFace format...\n",
      "Model Class     : v7_qwrky\n",
      "Model Source    : /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_qwrky/.model/qwrky7-7B.pth\n",
      "Tokenizer Type  : auto\n",
      "Output Directory: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_qwrky/.hf_build/qwrky7-7B/\n",
      "-----------------------------\n",
      "Building rwkv_block into HF code ...\n",
      "Loading model weights raw state ...\n",
      "Loading model config from weights ...\n",
      "-----------------------------\n",
      "Model Configuration:\n",
      "{'vocab_size': 152064, 'num_hidden_layers': 28, 'hidden_size': 3584, 'hidden_size_att': 512, 'hidden_size_ffn': 18944, 'head_size': 128, 'tmix_backend': 'auto', 'init_state_wkv': False, 'forward_chunk_size': 4096, 'dropout_rate': 0.0, 'use_cache': True, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': True, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': False, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': None, 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 0, 'pad_token_id': None, 'eos_token_id': 0, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '', '_commit_hash': None, '_attn_implementation_internal': None, '_attn_implementation_autoset': False, 'transformers_version': None, 'padding_idx': 151643, 'rms_norm_eps': 1e-06, 'attention_bias': True, 'attention_output_bias': False, 'layer_id': None, 'device': None, 'dtype': None}\n",
      "-----------------------------\n",
      "Loading model class instance ...\n",
      "Detected Tokenizer Type: qwen2\n",
      "Loading model state into class ...\n",
      "-----------------------------\n",
      "Saving tokenizer files ...\n",
      "Saving model code files ...\n",
      "Saving model weight files ...\n",
      "Patching configuration ...\n",
      "-----------------------------\n",
      "Successfully converted RWKV model to HuggingFace format\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Empty the output directory, if it exists\n",
    "if os.path.isdir(output_dir):\n",
    "    import shutil\n",
    "    print(\"Removing existing output directory\")\n",
    "    shutil.rmtree(output_dir)\n",
    "    \n",
    "# Run the hf_builder.py\n",
    "!python3 \"$project_dir/hf_builder/hf_builder.py\" --model_class \"v7_qwrky\" \"$model_file\" \"$output_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic HELLO WORLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.36it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully\n",
      "Running on device: cuda\n",
      "---------------------------------\n",
      "Prompt: HELLO WORLD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: HELLO WORLD\n",
      "---------\n",
      "\n",
      "This is a simple hello world program in C. It prints \"Hello, World!\" to\n",
      "---------------------------------\n",
      "Prompt: \n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\n",
      "Generated text: \n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. The dragons were able to communicate with the researchers, and they explained that they had been living in the\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the built model, using the transformers library\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "# Validating the config and tokenizer are built correctly\n",
    "config = AutoConfig.from_pretrained(output_dir, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir, trust_remote_code=True)\n",
    "\n",
    "# Move the model to the GPU\n",
    "RUN_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Build the model itself\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir, trust_remote_code=True, tmix_backend=\"fused_fla\", device=RUN_DEVICE)\n",
    "model.to(RUN_DEVICE)\n",
    "print(\"Model and tokenizer loaded successfully\")\n",
    "\n",
    "# Print the device being used\n",
    "print(\"Running on device:\", RUN_DEVICE)\n",
    "\n",
    "# Lets generate some text, using the model on the GPU\n",
    "dragon_prompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n",
    "hellow_prompt = \"HELLO WORLD\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"---------------------------------\")\n",
    "    print(f\"Prompt: {hellow_prompt}\")\n",
    "    inputs = tokenizer(hellow_prompt, return_tensors=\"pt\").to(RUN_DEVICE)\n",
    "    outputs = model.generate(**inputs)\n",
    "    print(\"Generated text:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "    print(\"---------------------------------\")\n",
    "    print(f\"Prompt: {dragon_prompt}\")\n",
    "    inputs = tokenizer(dragon_prompt, return_tensors=\"pt\").to(RUN_DEVICE)\n",
    "    outputs = model.generate(**inputs)\n",
    "    print(\"Generated text:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU validation testing (smaller set)\n",
    "**(this is not a substitute for lm-eval-harness : the score is counted differently)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Using HF model tokenizer: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_qwrky/.hf_build/qwrky7-7B/\n",
      "## Building MMLU cached dataset (n_shot=0,tokenizer=Qwen2TokenizerFast): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-val-t_Qwen2TokenizerFast-n_0-p_0-c_16-r1.pth\n",
      "## Building dataset for validation subject (n_shot=0): all\n",
      "## Dataset is ready for validation subject (n_shot=0): all\n",
      "## Longest prompt token length: 768\n",
      "## Padding to target prompt length: 768\n",
      "## Dataset is padded for validation subject (n_shot=0): all\n",
      "## Saving MMLU dataset cache (n_shot=0,tokenizer=Qwen2TokenizerFast): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-val-t_Qwen2TokenizerFast-n_0-p_0-c_16-r1.pth\n",
      "## Done: Dataset has been built and cached\n"
     ]
    }
   ],
   "source": [
    "# MMLU tester directory\n",
    "mmlu_test_dir = os.path.join(project_dir, \"test/mmlu\")\n",
    "\n",
    "# Run the test dataset builder, optional:  --use_validation_set\n",
    "!python3 {mmlu_test_dir}/BuildTestMMLU.py --hf_model \"$output_dir\" --n_shot 0 --use_validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_qwrky/.hf_build/qwrky7-7B/\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:00<00:00,  7.70it/s]\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=Qwen2TokenizerFast): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-val-t_Qwen2TokenizerFast-n_0-p_0-c_16-r1.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : all (count=1531, batches=192) ...\n",
      "Using /home/recursal/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py312_cu121/state_wind_backstepping/build.ninja...\n",
      "/home/recursal/miniconda3/envs/py-3-12/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module state_wind_backstepping...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module state_wind_backstepping...\n",
      "#### all - accuracy=0.6440 , probability=0.5409\n",
      "------------------------------------------------\n",
      "### MMLU overall test result : accuracy=0.6440 , probability=0.5409\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the cuda kernel\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 8 --n_shot 0 --use_validation_set --tmix_backend \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "## Loading HF model: /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/v7_qwrky/.hf_build/qwrky7-7B/\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:01<00:00,  3.93it/s]\n",
      "------------------------------------------------\n",
      "## Preparing the dataset\n",
      "## Loading MMLU cached dataset (n_shot=0,tokenizer=Qwen2TokenizerFast): /home/recursal/rwkv-prj/layerwise-trainer/block/RWKV_block/test/mmlu/.mmlu_cache/mmlu-val-t_Qwen2TokenizerFast-n_0-p_0-c_16-r1.pth\n",
      "## Done: Dataset has been built and cached\n",
      "------------------------------------------------\n",
      "## Starting the MMLU test ...\n",
      "### Running MMLU test : all (count=1531, batches=192) ...\n"
     ]
    }
   ],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 8 --n_shot 0 --use_validation_set --tmix_backend \"triton\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 8 --n_shot 0 --use_validation_set --tmix_backend \"triton_bighead\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 8 --n_shot 0 --use_validation_set --tmix_backend \"fla\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 8 --n_shot 0 --use_validation_set --tmix_backend \"fla_fused\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU testing \n",
    "**(this is not a substitute for lm-eval-harness : the score is counted differently)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMLU tester directory\n",
    "mmlu_test_dir = os.path.join(project_dir, \"test/mmlu\")\n",
    "\n",
    "# Run the test dataset builder, optional:  --use_validation_set\n",
    "!python3 {mmlu_test_dir}/BuildTestMMLU.py --hf_model \"$output_dir\" --n_shot 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the HF based MMLU tester, with the cuda kernel (modified)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 8 --n_shot 0 --tmix_backend \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel (modified)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 8 --n_shot 0 --tmix_backend \"triton\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel (modified)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 8 --n_shot 0 --tmix_backend \"triton_bighead\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel (modified)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 8 --n_shot 0 --tmix_backend \"fla\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the HF based MMLU tester, with the triton kernel (modified)\n",
    "!python3 {mmlu_test_dir}/RunTestMMLU.py \"$output_dir\" --batch_size 8 --n_shot 0 --tmix_backend \"fla_fused\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py-3-12]",
   "language": "python",
   "name": "conda-env-py-3-12-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
